{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a5aa27b3-30f9-4cd4-80e2-b9dd6460188a",
   "metadata": {},
   "source": [
    "# Python Text Analysis: Topic Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ece745f-2399-481f-9677-dc68ae158715",
   "metadata": {},
   "source": [
    "We've considered so far how to create numerical representations of words. We've even performed sentiment classification with those numerical representations. How else can we leverage these numerical representations to elucidate structure from natural language?\n",
    "\n",
    "In this session, we're going to discuss *topic modeling*. In topic modeling, we aim to discover how the documents in a corpus may be modeled as a function of specific topics. This is not the same thing as direct clustering, though, in which we might directly assign each document to a particular cluster.\n",
    "\n",
    "Consider genre classification. Some books may neatly fall into one genre, such as mystery, science fiction, etc. However, other books may be considered as incorporating multiple genres. You might have a fantasy novel which has mystery components to it, or a romance novel set in the future. In these cases, we don't want to cluster the fantasy novel into a \"fantasy\" bucket, and the romance novel in a \"romance\" bucket. We'd instead like to have some measure of assigning various topics, with different magnitudes to documents. This is the goal of topic modeling.\n",
    "\n",
    "We will use two approaches to perform topic modeling on the same corpus: non-negative matrix factorization, and latent dirichlet allocation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb1b9bf-6ed6-4abb-aab1-61d9fc209587",
   "metadata": {},
   "source": [
    "# Dataset: 20 Newsgroups\n",
    "\n",
    "We will be using a new dataset called the **20 Newsgroups** dataset. You can find the original page for this dataset [here](http://qwone.com/~jason/20Newsgroups/).\n",
    "\n",
    "This dataset is comprised of around 18000 newsgroups posts on 20 topics. The split between the train and test set is based upon a messages posted before and after a specific date. The news groups are as follows, with specific labels indicated:\n",
    "\n",
    "* *Computers*\n",
    "    * comp.graphics\n",
    "    * comp.os.ms-windows.misc\n",
    "    * comp.sys.ibm.pc.hardware\n",
    "    * comp.sys.mac.hardware\n",
    "    * comp.windows.x\n",
    "* *Recreation*\n",
    "    * rec.autos\n",
    "    * rec.motorcycles\n",
    "    * rec.sport.baseball\n",
    "    * rec.sport.hockey\n",
    "* *Science*\n",
    "    * sci.crypt\n",
    "    * sci.electronics\n",
    "    * sci.med\n",
    "    * sci.space\n",
    "* *Miscellaneous*\n",
    "    * misc.forsale\n",
    "* *Politics*\n",
    "    * talk.politics.misc\n",
    "    * talk.politics.guns\n",
    "    * talk.politics.mideast\n",
    "* *Religion*\n",
    "    * talk.religion.misc\n",
    "    * alt.atheism\n",
    "    * soc.religion.christian\n",
    "    \n",
    "Let's begin by importing the dataset. We'll use `scikit-learn` to do so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c10c372a-1297-4846-b767-1c4ed2be41e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e81a98-7080-4997-bc57-653f3c66a078",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import fetcher function\n",
    "from sklearn.datasets import fetch_20newsgroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f8c845-a42d-42eb-84a6-a48614c91526",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Always check the documentation!\n",
    "data, labels = fetch_20newsgroups(\n",
    "    subset='all',\n",
    "    shuffle=True,\n",
    "    random_state=1,\n",
    "    remove=(\"headers\", \"footers\", \"quotes\"),\n",
    "    return_X_y=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9da61bd-2425-4d9d-8812-79d2290644f7",
   "metadata": {},
   "source": [
    "Let's take a look at some of the data samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f720479-b0b0-4823-b3db-350727159985",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data[1000])\n",
    "print('--------')\n",
    "print(data[2000])\n",
    "print('--------')\n",
    "print(data[3000])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f90f2cb3-fc83-4c5b-b90a-f100e17da6b3",
   "metadata": {},
   "source": [
    "If we take a look at the labels, we see that they're integers, each specifying one of the 20 possible classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d35d8e9-8f3b-43de-84dc-3bcc2f0085e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.unique(labels))\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca437a87-294f-4c3b-8cb1-bed40137973f",
   "metadata": {},
   "source": [
    "We can access the corresponding names of these labels by using a different keyword argument in the original `fetch_20newsgroups` call:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df7fdc8-5e1b-461d-8bde-50997763c986",
   "metadata": {},
   "outputs": [],
   "source": [
    "newsgroups = fetch_20newsgroups(\n",
    "    subset='all',\n",
    "    shuffle=True,\n",
    "    random_state=1,\n",
    "    remove=(\"headers\", \"footers\", \"quotes\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f0e555-f7d7-42e4-b8c6-56219306a383",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(newsgroups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "592d2040-3685-4a17-bb67-51d193d40e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "newsgroups.target_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c901796-78b1-41a0-bda4-83b2f76aa7f8",
   "metadata": {},
   "source": [
    "# Non-negative Matrix Factorization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4870152-ec3e-4ca6-809f-1d6214e4d56c",
   "metadata": {},
   "source": [
    "Non-negative matrix factorization (NMF) is a dimensionality reduction technique that can be used to perform topic modeling. What does it mean? Non-negative implies we're not working with any negative numbers, and matrix factorization implies we're taking a matrix, and breaking it down into \"factors\".\n",
    "\n",
    "The matrix under consideration is going to be some numerical representation of the text. We've already considered one - the document term matrix! Specifically, we're going to build a TF-IDF matrix from the data - let's call this matrix $V$. Let's remind ourselves what $V$ is: it's a $D\\times T$ matrix, where $D$ is the number of documents, and $T$ is the number or terms, or tokens. \n",
    "\n",
    "![NMF](../images/nmf.png)\n",
    "\n",
    "The goal in NMF is to write this matrix as a product of two matrices,\n",
    "\n",
    "$$\n",
    "V \\approx WH\n",
    "$$\n",
    "\n",
    "where $W$ is a $D\\times K$ matrix and $H$ is a $K\\times T$ matrix. So, in matrix $W$, we can still consider rows as corresponding to documents, and in $H$, we can think of columns as corresponding to terms. But what about $K$, the inner dimension?\n",
    "\n",
    "We can think of $K$ as enumerating *topics*. If $K$ corresponds to topics, then each row of $H$ corresponds to a different topic. We can interpret $H$ as enumerating what contribution each *term* makes to each *topic*. For example, if the first row of the NMF only has non-zero entries for terms `soccer`, `basketball`, and `baseball`, we might reasonably conclude that the topic corresponds to \"sports\". The numbers for each entry indicate the contribution of that term to the topic - so if the topic is mainly baseball, it might have a higher value in that entry.\n",
    "\n",
    "What does this mean for $W$? It's detailing how each *document* (rows) break down into *topics* (columns). So, we can think of NMF as estimating the contribution different topics to a specific document. In newsgroups, we might expect there to be a large contribution from that \"sports\" topic above into the samples labeled as \"rec.sports.baseball\".\n",
    "\n",
    "The breakdown of the original matrix into $W$ and $H$ can only be interpreted if there are no negative entries in any matrix. We already know a TF-IDF DTM is going to be non-negative. If we guarantee that $W$ and $H$ are as well, we can quite literally think of this factorization is creating building blocks for each document!\n",
    "\n",
    "There's a small issue here: we can get the breakdown of documents into topics, but it's up to us to \"interpret\" what the topics might be. This can be tricky business, as we might see. Furthermore, we don't even know how many topics we should pick! There are procedures to identify a good number of topics, but at some level it is subjective."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "175eba48-0a62-4daf-95d4-b9168e4cca46",
   "metadata": {},
   "source": [
    "Let's try fitting an NMF to the newsgroups data. First, we need to use `TfidfVectorizer` to transform the data into a document term matrix (remember how to do this?)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4325121-5c93-44f5-90c3-fd87df7ce095",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Is this a good scenario to be removing stop words?\n",
    "n_tokens = 1000\n",
    "\n",
    "vectorizer = TfidfVectorizer(\n",
    "    max_df=0.95,\n",
    "    min_df=2,\n",
    "    max_features=n_tokens,\n",
    "    stop_words=\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "334a2faf-ea15-48e2-bbc9-8a80675c83c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform vectorizing\n",
    "tfidf = vectorizer.fit_transform(data)\n",
    "tokens = vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d83112a-4563-4270-827c-610a388ffba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many samples, and how many tokens?\n",
    "tfidf.todense().shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cfd70fd-3b2e-4f8b-854c-34c59345193f",
   "metadata": {},
   "source": [
    "Let's look at the tokens with highest TF-IDF scores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09526020-84da-438e-a46b-afc765ace3ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_df = pd.DataFrame(\n",
    "    tfidf.todense(),\n",
    "    columns=tokens)\n",
    "tfidf_df.sum(axis=0).sort_values(ascending=False).head(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fadbd0bb-c2bd-4d58-9291-c17ad769c1ce",
   "metadata": {},
   "source": [
    "We can perform NMF using the `NMF` module from `scikit-learn`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b33adb99-30bb-4522-a8be-ae9f58fd1ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "765e4544-b709-4cba-ad1e-86625839d5c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_components = 10\n",
    "random_state = 1\n",
    "\n",
    "nmf = NMF(\n",
    "    n_components=n_components,\n",
    "    random_state=random_state,\n",
    "    alpha=0.1,\n",
    "    l1_ratio=0.5,\n",
    "    init='nndsvda',\n",
    "    max_iter=500).fit(tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68485e19-089a-463c-af90-00b2ebefb70d",
   "metadata": {},
   "source": [
    "We can take a look at the $H$ matrix, or the topics, by examining the `components_` member variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c698fe2-8671-426d-9d65-d7962aff07f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nmf.components_.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8005931-59a7-45a1-8904-ed7784885e97",
   "metadata": {},
   "source": [
    "Let's take a peek at the distribution of values in the first topic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c1b83a-0be1-412d-8b16-9178cece4417",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1)\n",
    "\n",
    "ax.bar(x=np.arange(n_tokens), height=nmf.components_[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec26cb5e-9ff0-41bc-a827-5a1fbe93ca93",
   "metadata": {},
   "source": [
    "This is nice, but it'd be nice to look at the top tokens - the large spikes we see there - and ignore all the smaller contributions to a topic. We'll use a plotter function which will nicely show the largest contributions to each topic, as well as what the corresponding tokens are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "003e9781-9a3b-4dbf-a4c9-55f60839ece0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_top_words(model, feature_names, n_top_words=10, n_row=2, n_col=5):\n",
    "    fig, axes = plt.subplots(n_row, n_col, figsize=(3 * n_col, 5 * n_row), sharex=True)\n",
    "    axes = axes.flatten()\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        top_features_ind = topic.argsort()[: -n_top_words - 1 : -1]\n",
    "        top_features = [feature_names[i] for i in top_features_ind]\n",
    "        weights = topic[top_features_ind]\n",
    "\n",
    "        ax = axes[topic_idx]\n",
    "        ax.barh(top_features, weights, height=0.7)\n",
    "        ax.set_title(f\"Topic {topic_idx +1}\", fontdict={\"fontsize\": 20})\n",
    "        ax.invert_yaxis()\n",
    "        ax.tick_params(axis=\"both\", which=\"major\", labelsize=20)\n",
    "\n",
    "        for i in \"top right left\".split():\n",
    "            ax.spines[i].set_visible(False)\n",
    "\n",
    "    plt.subplots_adjust(top=0.90, bottom=0.05, wspace=0.90, hspace=0.3)\n",
    "\n",
    "    return fig, axes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de835650-7d53-4570-8281-3b4c4b3e482e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plot_top_words(nmf, tokens)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf55084e-a174-4356-a6dc-bac12b81f45c",
   "metadata": {},
   "source": [
    "What can we tell from this? For one, Topic 3 looks very much like a topic related to religion; Topic 4 looks like it's about sports; Topic 7 looks like it might be related to computers; and Topic 10 looks realted to politics (specifically, Israeli or generally Middle Eastern politics).\n",
    "\n",
    "Some of the topics look a little bit harder to interpret, and that comes with the territory in topic modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f51840b-147c-4946-8b1f-4709b74895e4",
   "metadata": {},
   "source": [
    "We were able to get the $H$ matrix, but what about the $W$ matrix? To get this, we need to *transform* the DTM into the *basis* created by the NMF factorization. We can do this with the `transform` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c7aa63b-d2a5-444d-a8f8-7455887fe882",
   "metadata": {},
   "outputs": [],
   "source": [
    "W = nmf.transform(tfidf)\n",
    "print(W.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "281d03cb-b5d2-454c-88e9-8f333815ef5c",
   "metadata": {},
   "source": [
    "Let's take a look at a random entry:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d67fec-5c41-4fa8-806d-37bbdefe9bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "entry = 999\n",
    "print(data[999])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e39129-137d-443d-93e4-f2ec4f6a800e",
   "metadata": {},
   "source": [
    "The accompanying label for this entry can be obtained from the `newsgroups` variable we created earlier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77bb79d4-8eb5-4e39-bde3-957f778db917",
   "metadata": {},
   "outputs": [],
   "source": [
    "newsgroups.target_names[labels[999]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24da7ff8-501b-4f81-9f1b-116f4426de92",
   "metadata": {},
   "source": [
    "Let's examine the breakdown of this entry by topic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "881c7cce-984c-4b1a-9221-05b5ab750300",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1)\n",
    "ax.bar(x=np.arange(n_components), height=W[999])\n",
    "ax.set_xticks(np.arange(n_components))\n",
    "ax.set_xticklabels(np.arange(n_components) + 1)\n",
    "ax.set_xlabel('Topic', fontsize=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e02528-7d53-4365-9589-1368ce0e5d50",
   "metadata": {},
   "source": [
    "The biggest contribution is topic 7, which makes some sense: it generally seems to deal with computers. Notice that many of the topics have been zeroed out entirely - this is a product of enforcing *sparsity* in the model, and it stems from the `alpha` parameter in the NMF."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aafdde48-56c6-4550-ad7a-f6d100e95694",
   "metadata": {},
   "source": [
    "## Challenge 1: Exploring Hyperparameters in NMF\n",
    "\n",
    "The choice of 10 components was somewhat arbitrary. It was something we had to do before we could proceed with fitting the model to the data. This is what's known as a *hyperparameter*. There are other hyperparameters in the `NMF`. For example, the `alpha` values specifies to what degree we should force values to be set equal to zero.\n",
    "\n",
    "Try fitting the NMF with other variations of hyperparameters, and plot the resulting topics using the `plot_top_words` function. What do you notice?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b4b4900-e5a5-4774-adff-7fd3f3146b9f",
   "metadata": {},
   "source": [
    "# Latent Dirichlet Allocation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c661bd4f-f4bb-4276-ab2e-aac8622450ee",
   "metadata": {},
   "source": [
    "![lda](../images/lda.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "185447cb-f59e-4dfe-9a8a-7c350f21e1c2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
