{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a5aa27b3-30f9-4cd4-80e2-b9dd6460188a",
   "metadata": {},
   "source": [
    "# Python Text Analysis: Topic Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ece745f-2399-481f-9677-dc68ae158715",
   "metadata": {},
   "source": [
    "We've considered so far how to create numerical representations of words. We've even performed sentiment classification with those numerical representations. How else can we leverage these numerical representations to elucidate structure from natural language?\n",
    "\n",
    "In this session, we're going to discuss *topic modeling*. In topic modeling, we aim to discover how the documents in a corpus may be modeled as a function of specific topics. This is not the same thing as direct clustering, though, in which we might directly assign each document to a particular cluster.\n",
    "\n",
    "Consider genre classification. Some books may neatly fall into one genre, such as mystery, science fiction, etc. However, other books may be considered as incorporating multiple genres. You might have a fantasy novel which has mystery components to it, or a romance novel set in the future. In these cases, we don't want to cluster the fantasy novel into a \"fantasy\" bucket, and the romance novel in a \"romance\" bucket. We'd instead like to have some measure of assigning various topics, with different magnitudes to documents. This is the goal of topic modeling.\n",
    "\n",
    "We will use two approaches to perform topic modeling on the same corpus: non-negative matrix factorization, and latent dirichlet allocation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb1b9bf-6ed6-4abb-aab1-61d9fc209587",
   "metadata": {},
   "source": [
    "# Dataset: 20 Newsgroups\n",
    "\n",
    "We will be using a new dataset called the **20 Newsgroups** dataset. You can find the original page for this dataset [here](http://qwone.com/~jason/20Newsgroups/).\n",
    "\n",
    "This dataset is comprised of around 18000 newsgroups posts on 20 topics. The split between the train and test set is based upon a messages posted before and after a specific date. The news groups are as follows, with specific labels indicated:\n",
    "\n",
    "* *Computers*\n",
    "    * comp.graphics\n",
    "    * comp.os.ms-windows.misc\n",
    "    * comp.sys.ibm.pc.hardware\n",
    "    * comp.sys.mac.hardware\n",
    "    * comp.windows.x\n",
    "* *Recreation*\n",
    "    * rec.autos\n",
    "    * rec.motorcycles\n",
    "    * rec.sport.baseball\n",
    "    * rec.sport.hockey\n",
    "* *Science*\n",
    "    * sci.crypt\n",
    "    * sci.electronics\n",
    "    * sci.med\n",
    "    * sci.space\n",
    "* *Miscellaneous*\n",
    "    * misc.forsale\n",
    "* *Politics*\n",
    "    * talk.politics.misc\n",
    "    * talk.politics.guns\n",
    "    * talk.politics.mideast\n",
    "* *Religion*\n",
    "    * talk.religion.misc\n",
    "    * alt.atheism\n",
    "    * soc.religion.christian\n",
    "    \n",
    "Let's begin by importing the dataset. We'll use `scikit-learn` to do so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c10c372a-1297-4846-b767-1c4ed2be41e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e81a98-7080-4997-bc57-653f3c66a078",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import fetcher function\n",
    "from sklearn.datasets import fetch_20newsgroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f8c845-a42d-42eb-84a6-a48614c91526",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Always check the documentation!\n",
    "data, labels = fetch_20newsgroups(\n",
    "    subset='train',\n",
    "    shuffle=True,\n",
    "    random_state=1,\n",
    "    remove=(\"headers\", \"footers\", \"quotes\"),\n",
    "    return_X_y=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9da61bd-2425-4d9d-8812-79d2290644f7",
   "metadata": {},
   "source": [
    "Let's take a look at some of the data samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f720479-b0b0-4823-b3db-350727159985",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data[1000])\n",
    "print('--------')\n",
    "print(data[2000])\n",
    "print('--------')\n",
    "print(data[3000])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f90f2cb3-fc83-4c5b-b90a-f100e17da6b3",
   "metadata": {},
   "source": [
    "If we take a look at the labels, we see that they're integers, each specifying one of the 20 possible classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d35d8e9-8f3b-43de-84dc-3bcc2f0085e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.unique(labels))\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca437a87-294f-4c3b-8cb1-bed40137973f",
   "metadata": {},
   "source": [
    "We can access the corresponding names of these labels by using a different keyword argument in the original `fetch_20newsgroups` call:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df7fdc8-5e1b-461d-8bde-50997763c986",
   "metadata": {},
   "outputs": [],
   "source": [
    "newsgroups = fetch_20newsgroups(\n",
    "    subset='train',\n",
    "    shuffle=True,\n",
    "    random_state=1,\n",
    "    remove=(\"headers\", \"footers\", \"quotes\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f0e555-f7d7-42e4-b8c6-56219306a383",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(newsgroups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "592d2040-3685-4a17-bb67-51d193d40e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "newsgroups.target_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c901796-78b1-41a0-bda4-83b2f76aa7f8",
   "metadata": {},
   "source": [
    "# Non-negative Matrix Factorization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4870152-ec3e-4ca6-809f-1d6214e4d56c",
   "metadata": {},
   "source": [
    "Non-negative matrix factorization (NMF) is a dimensionality reduction technique that can be used to perform topic modeling. It was first [introduced](https://www.nature.com/articles/44565) by Lee and Seung in 1999.\n",
    "\n",
    "What does *non-negative matrix factorization* mean? Non-negative implies we're not working with any negative numbers, and matrix factorization implies we're taking a matrix, and breaking it down into \"factors\".\n",
    "\n",
    "The matrix under consideration is going to be some numerical representation of the text. We've already considered one - the document term matrix! Specifically, we're going to build a TF-IDF matrix from the data - let's call this matrix $V$. Let's remind ourselves what $V$ is: it's a $D\\times T$ matrix, where $D$ is the number of documents, and $T$ is the number or terms, or tokens. \n",
    "\n",
    "![NMF](../images/nmf.png)\n",
    "\n",
    "The goal in NMF is to write this matrix as a product of two matrices,\n",
    "\n",
    "$$\n",
    "V \\approx WH\n",
    "$$\n",
    "\n",
    "where $W$ is a $D\\times K$ matrix and $H$ is a $K\\times T$ matrix. So, in matrix $W$, we can still consider rows as corresponding to documents, and in $H$, we can think of columns as corresponding to terms. But what about $K$, the inner dimension?\n",
    "\n",
    "We can think of $K$ as enumerating *topics*. If $K$ corresponds to topics, then each row of $H$ corresponds to a different topic. We can interpret $H$ as enumerating what contribution each *term* makes to each *topic*. For example, if the first row of the NMF only has non-zero entries for terms `soccer`, `basketball`, and `baseball`, we might reasonably conclude that the topic corresponds to \"sports\". The numbers for each entry indicate the contribution of that term to the topic - so if the topic is mainly baseball, it might have a higher value in that entry.\n",
    "\n",
    "What does this mean for $W$? It's detailing how each *document* (rows) break down into *topics* (columns). So, we can think of NMF as estimating the contribution different topics to a specific document. In newsgroups, we might expect there to be a large contribution from that \"sports\" topic above into the samples labeled as \"rec.sports.baseball\".\n",
    "\n",
    "The breakdown of the original matrix into $W$ and $H$ can only be interpreted if there are no negative entries in any matrix. We already know a TF-IDF DTM is going to be non-negative. If we guarantee that $W$ and $H$ are as well, we can quite literally think of this factorization is creating building blocks for each document!\n",
    "\n",
    "There's a small issue here: we can get the breakdown of documents into topics, but it's up to us to \"interpret\" what the topics might be. This can be tricky business, as we might see. Furthermore, we don't even know how many topics we should pick! There are procedures to identify a good number of topics, but at some level it is subjective."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "175eba48-0a62-4daf-95d4-b9168e4cca46",
   "metadata": {},
   "source": [
    "Let's try fitting an NMF to the newsgroups data. First, we need to use `TfidfVectorizer` to transform the data into a document term matrix (remember how to do this?)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4325121-5c93-44f5-90c3-fd87df7ce095",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Is this a good scenario to be removing stop words?\n",
    "n_tokens = 1000\n",
    "\n",
    "vectorizer = TfidfVectorizer(\n",
    "    max_df=0.95,\n",
    "    min_df=2,\n",
    "    max_features=n_tokens,\n",
    "    stop_words=\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "334a2faf-ea15-48e2-bbc9-8a80675c83c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform vectorizing\n",
    "tfidf = vectorizer.fit_transform(data)\n",
    "tokens = vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d83112a-4563-4270-827c-610a388ffba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many samples, and how many tokens?\n",
    "tfidf.todense().shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cfd70fd-3b2e-4f8b-854c-34c59345193f",
   "metadata": {},
   "source": [
    "Let's look at the tokens with highest TF-IDF scores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09526020-84da-438e-a46b-afc765ace3ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_df = pd.DataFrame(\n",
    "    tfidf.todense(),\n",
    "    columns=tokens)\n",
    "tfidf_df.sum(axis=0).sort_values(ascending=False).head(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fadbd0bb-c2bd-4d58-9291-c17ad769c1ce",
   "metadata": {},
   "source": [
    "We can perform NMF using the `NMF` module from `scikit-learn`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b33adb99-30bb-4522-a8be-ae9f58fd1ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "765e4544-b709-4cba-ad1e-86625839d5c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_components = 10\n",
    "random_state = 1\n",
    "\n",
    "nmf = NMF(\n",
    "    n_components=n_components,\n",
    "    random_state=random_state,\n",
    "    alpha=0.1,\n",
    "    l1_ratio=0.5,\n",
    "    init='nndsvda',\n",
    "    max_iter=500).fit(tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68485e19-089a-463c-af90-00b2ebefb70d",
   "metadata": {},
   "source": [
    "We can take a look at the $H$ matrix, or the topics, by examining the `components_` member variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c698fe2-8671-426d-9d65-d7962aff07f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nmf.components_.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8005931-59a7-45a1-8904-ed7784885e97",
   "metadata": {},
   "source": [
    "Let's take a peek at the distribution of values in the first topic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c1b83a-0be1-412d-8b16-9178cece4417",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1)\n",
    "\n",
    "ax.bar(x=np.arange(n_tokens), height=nmf.components_[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec26cb5e-9ff0-41bc-a827-5a1fbe93ca93",
   "metadata": {},
   "source": [
    "This is nice, but it'd be nice to look at the top tokens - the large spikes we see there - and ignore all the smaller contributions to a topic. We'll use a plotter function which will nicely show the largest contributions to each topic, as well as what the corresponding tokens are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "003e9781-9a3b-4dbf-a4c9-55f60839ece0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_top_words(model, feature_names, n_top_words=10, n_row=2, n_col=5, normalize=False):\n",
    "    fig, axes = plt.subplots(n_row, n_col, figsize=(3 * n_col, 5 * n_row), sharex=True)\n",
    "    axes = axes.flatten()\n",
    "    components = model.components_\n",
    "    if normalize:\n",
    "        components = components / components.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    for topic_idx, topic in enumerate(components):\n",
    "        top_features_ind = topic.argsort()[: -n_top_words - 1 : -1]\n",
    "        top_features = [feature_names[i] for i in top_features_ind]\n",
    "        weights = topic[top_features_ind]\n",
    "\n",
    "        ax = axes[topic_idx]\n",
    "        ax.barh(top_features, weights, height=0.7)\n",
    "        ax.set_title(f\"Topic {topic_idx +1}\", fontdict={\"fontsize\": 20})\n",
    "        ax.invert_yaxis()\n",
    "        ax.tick_params(axis=\"both\", which=\"major\", labelsize=20)\n",
    "\n",
    "        for i in \"top right left\".split():\n",
    "            ax.spines[i].set_visible(False)\n",
    "\n",
    "    plt.subplots_adjust(top=0.90, bottom=0.05, wspace=0.90, hspace=0.3)\n",
    "\n",
    "    return fig, axes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de835650-7d53-4570-8281-3b4c4b3e482e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plot_top_words(nmf, tokens)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf55084e-a174-4356-a6dc-bac12b81f45c",
   "metadata": {},
   "source": [
    "What can we tell from this? For one, Topic 3 looks very much like a topic related to religion; Topic 4 looks like it's about sports; Topic 7 looks like it might be related to computers. Note that, if you used a different random seed, these topics might look different for you.\n",
    "\n",
    "Some of the topics look a little bit harder to interpret, and that comes with the territory in topic modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f51840b-147c-4946-8b1f-4709b74895e4",
   "metadata": {},
   "source": [
    "We were able to get the $H$ matrix, but what about the $W$ matrix? To get this, we need to *transform* the DTM into the *basis* created by the NMF factorization. We can do this with the `transform` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c7aa63b-d2a5-444d-a8f8-7455887fe882",
   "metadata": {},
   "outputs": [],
   "source": [
    "W = nmf.transform(tfidf)\n",
    "print(W.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "281d03cb-b5d2-454c-88e9-8f333815ef5c",
   "metadata": {},
   "source": [
    "Let's take a look at a random entry:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d67fec-5c41-4fa8-806d-37bbdefe9bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "entry = 999\n",
    "print(data[999])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e39129-137d-443d-93e4-f2ec4f6a800e",
   "metadata": {},
   "source": [
    "The accompanying label for this entry can be obtained from the `newsgroups` variable we created earlier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77bb79d4-8eb5-4e39-bde3-957f778db917",
   "metadata": {},
   "outputs": [],
   "source": [
    "newsgroups.target_names[labels[999]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24da7ff8-501b-4f81-9f1b-116f4426de92",
   "metadata": {},
   "source": [
    "Let's examine the breakdown of this entry by topic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "881c7cce-984c-4b1a-9221-05b5ab750300",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1)\n",
    "ax.bar(x=np.arange(n_components), height=W[999])\n",
    "ax.set_xticks(np.arange(n_components))\n",
    "ax.set_xticklabels(np.arange(n_components) + 1)\n",
    "ax.set_xlabel('Topic', fontsize=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e02528-7d53-4365-9589-1368ce0e5d50",
   "metadata": {},
   "source": [
    "What does this tell you?\n",
    "\n",
    "Notice that many of the topics have been zeroed out entirely - this is a product of enforcing *sparsity* in the model, and it stems from the `alpha` parameter in the NMF."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aafdde48-56c6-4550-ad7a-f6d100e95694",
   "metadata": {},
   "source": [
    "## Challenge 1: Exploring Hyperparameters in NMF\n",
    "\n",
    "The choice of 10 components was somewhat arbitrary. It was something we had to do before we could proceed with fitting the model to the data. This is what's known as a *hyperparameter*. There are other hyperparameters in the `NMF`. For example, the `alpha` values specifies to what degree we should force values to be set equal to zero.\n",
    "\n",
    "Try fitting the NMF with other variations of hyperparameters, and plot the resulting topics using the `plot_top_words` function. What do you notice?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b4b4900-e5a5-4774-adff-7fd3f3146b9f",
   "metadata": {},
   "source": [
    "# Latent Dirichlet Allocation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41687845-9442-4e27-bf87-4361b49ffc2e",
   "metadata": {},
   "source": [
    "Latent Dirichlet Allocation (LDA) is a Bayesian model that captures how specific topics can generate documents. It was [introduced](https://jmlr.csail.mit.edu/papers/v3/blei03a.htmlhttps://jmlr.csail.mit.edu/papers/v3/blei03a.html) in machine learning by Blei et al. It is one of the oldest models applied to perform topic modeling.\n",
    "\n",
    "One significant difference between LDA and NMF is that LDA is a *generative* model. This means that it can be used to *generate* new documents, by sampling from it. Assume we have a number of topics $T$. Then, we generate a new document as follows:\n",
    "\n",
    "1. Choose a number of words $N$ according to a Poisson distribution.\n",
    "2. Choose a vector of values $\\boldsymbol{\\theta}=(\\theta_1, \\theta_2, \\ldots, \\theta_T)$ from a Dirichlet distribution. The details of a Dirichlet distribution aren't too important other than that it guarantees all of the $\\theta_i$ add up to 1, and are positive. So, we can think of the $\\theta_i$ as proportions, or probabilities.\n",
    "3. For each of the $N$ words $w_n$:\n",
    "- Choose a topic $t_n$ according to a Multinomial distribution following $\\boldsymbol{\\theta}$.\n",
    "- Choose a word $w_n$ from a probability distribution $p(w_n|t_n)$ conditioned on $t_n$. This probability distribution is another Multinomial distribution.\n",
    "\n",
    "LDA does not model the order of the words, so in the end, it produces a collection of words - just like the bag of words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c661bd4f-f4bb-4276-ab2e-aac8622450ee",
   "metadata": {},
   "source": [
    "![lda](../images/lda.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f18f3d82-7c66-4fd1-b3ee-2478adc10627",
   "metadata": {},
   "source": [
    "There's a lot of variables there, so let's consider a concrete example. Let's suppose we have two topics: soccer and basketball. These are $t_1$ and $t_2$. \n",
    "\n",
    "Some topics are more likely to contains words than others. For example, soccer is more likely to contain `liverpool` and `freekick`, but probably not `nba`. Basketball meanwhile will very likely contain `rebound` and `nba`. Furthermore, even though it's unlikely, a soccer topic might still refer to the `nba`. This unlikeliness is captured through the probabilities assigned in the distribution $p(w_n|t_n)$.\n",
    "\n",
    "Next, each document might consist of multiple \"proportions\" of topics. We've already seen this in NMF, only this time, LDA captures this via a probability distribution rather than a matrix operation. So, Document 1 might mainly be about Soccer, and not really reference basketball - this would be reflected in the probabilities $\\boldsymbol{\\theta}=(0.9, 0.1)$. Meanwhile, another document might equally reference soccer and basketball, so we'd need a different set of parameters $\\boldsymbol{\\theta}=(0.5, 0.5)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b01a71-66d7-4b06-a895-f258794c531d",
   "metadata": {},
   "source": [
    "Once again, we're going to use `scikit-learn` to perform LDA. This time, however, we'll use a `CountVectorizer`, since LDA explicitly models *counts*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5840f2e-3d16-40b2-a005-ce7b38c7fa6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a CountVectorizer\n",
    "n_tokens = 1000\n",
    "count_vectorizer = CountVectorizer(\n",
    "    max_df=0.95,\n",
    "    min_df=2,\n",
    "    max_features=n_tokens,\n",
    "    stop_words=\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c02cc0-e3c9-4d0f-bd11-48a698df8a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit and transform CountVectorizer\n",
    "counts = count_vectorizer.fit_transform(data)\n",
    "print(counts.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "332778ad-4b3a-4e29-9dde-d1fea1f69769",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = count_vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68a4fdb8-1c6f-486b-bece-42208b918970",
   "metadata": {},
   "source": [
    "This DTM is going to look very similar to the previous DTM, so let's proceed to the LDA fitting!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b53abe0-e419-45e7-9b51-14b276a25d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e65d6b-3d87-463b-a56c-e5d0ccf48d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_components = 10\n",
    "random_state = 0\n",
    "\n",
    "lda = LatentDirichletAllocation(\n",
    "    n_components=n_components,\n",
    "    max_iter=5,\n",
    "    learning_method=\"online\", # Use when dataset is large\n",
    "    learning_offset=50.0, \n",
    "    random_state=random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea61cb9e-7c02-4766-b958-263e4a36b739",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the LDA model!\n",
    "lda.fit(counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7362b9d7-d58d-423f-8940-6dc5cd726b02",
   "metadata": {},
   "source": [
    "How can we analyze the trained model? The `lda` object also comes with a `components_` variable, which corresponds to the topic word distribution. Let's plot these values using the function we created above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5ebb041-5b6d-4f99-8518-50cd8ee43540",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This time, we're normalizing - what does this do?\n",
    "fig, axes = plot_top_words(lda, tokens, normalize=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d404af01-7ee8-4125-a86e-3e9244f84046",
   "metadata": {},
   "source": [
    "## Challenge 3: Exploring Hyperparameters in LDA\n",
    "\n",
    "As in the case of NMF, try performing LDA with other variations of hyperparameters, and plot the resulting topics using the `plot_top_words` function. Use the [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.LatentDirichletAllocation.html) as a guide to choose different hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e8eb97-027f-47db-a04a-4dea8d1b6ecc",
   "metadata": {},
   "source": [
    "# Topic Modeling as Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cabd263-85c1-46d4-9beb-ab7095487487",
   "metadata": {},
   "source": [
    "In both NMF and LDA, we broke down the documents into topics. This was, in effect, a *change in representation*. We went from a DTM representation, to a representation of *topics*. \n",
    "\n",
    "Because there are fewer topics than there are tokens, we can think of this as a *dimensionality reduction*. This is desirable for several reasons, the main one being that it's easier to interpret, say, 10 dimensions than it is to interpret 1000.\n",
    "\n",
    "This is computationally true, as well: once we get to higher dimensions, it's harder to compare different vectors with each other, because they generally end up all close to orthogonal. This is known as the *curse of dimensionality*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29671450-9beb-4418-92cf-a6f146212a2b",
   "metadata": {},
   "source": [
    "Let's first transform the counts into the topic representation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e28b5b78-2d74-4de2-b14c-8f434530f55e",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_representation = lda.transform(counts)\n",
    "topic_representation.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "641fe529-7bdf-49d2-9de7-1a43d89dd197",
   "metadata": {},
   "source": [
    "We're going to use the cosine similarity to calculate the similarity between pairs of documents (remember this from Word Embeddings?). `scikit-learn` has a `cosine_similarity` function we can use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce068c6a-aafb-4fdf-b1fe-5b742e472a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d62d3dde-1f72-45b9-a403-a45d25c5cf14",
   "metadata": {},
   "source": [
    "Let's first calculate the similarity of the first two documents in the term representation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30c68401-c26d-404e-b39c-03d97c5f21f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_similarity(counts[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f59c32e9-7813-4839-8b2d-91f81d4ec504",
   "metadata": {},
   "source": [
    "Not similar at all! Let's try in the topic representation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e27063-66bb-409a-83be-f34b685aff2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_similarity(topic_representation[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "320a8bb4-4075-4a9b-ac9b-ff0acbb0a0e3",
   "metadata": {},
   "source": [
    "The similarity is higher, but not that higher. What about the rest of the documents?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abb994b3-d853-4660-b3bf-84af4f82e00f",
   "metadata": {},
   "source": [
    "## Challenge 3: Finding Similar Documents\n",
    "\n",
    "Calculate the cosine similarity between all pairs of documents, and find the two documents whose cosine similarity is the highest. What are these documents? Do they seem similar?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
