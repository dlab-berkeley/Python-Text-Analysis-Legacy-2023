{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ffcdc86-76ef-453f-a162-7741c9c23b8c",
   "metadata": {},
   "source": [
    "# Python Text Analysis: Fundamentals, Part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e462b819-abfc-4a0b-93a3-3e1aa2be9d24",
   "metadata": {},
   "source": [
    "In the previous lesson, we learned how to use several packages to preprocess text. However, we never moved beyond the *text representation* - we only manipulated the representation itself. If we want to perform computational analysis on the text, we will need to devise approaches to convert the text into a *numeric representation*.\n",
    "\n",
    "In this lesson, we'll explore one of the simplest ways to generate a numeric representation from text: the **bag-of-words**. With this numeric representation, we'll build a simple classifier and explore what the classifier tells us. At the heart of the bag-of-words approach lies the hypothesis that the presence and frequency of specific tokens is informative about the semantics and sentiment behind the text.\n",
    "\n",
    "We'll make heavy use of the `scikit-learn` package to do so, as it provides a nice framework for constructing the numeric representations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef4dbc3-52dd-4dc9-8d0d-21ad31808d09",
   "metadata": {},
   "source": [
    "# The Airline Tweets Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcd83f11-7239-425a-b505-3f3f3425079d",
   "metadata": {},
   "source": [
    "We'll work with a dataset consisting of tweets about US airlines. Each sample is a different tweet, which was posted at a specific airline. These tweets may express positive, neutral, or negative \"sentiment\". Our eventual goal will be to predict the sentiment of the tweet given its text.\n",
    "\n",
    "The dataset was collected by Crowdflower, which they then made public through Kaggle. We've already downloaded it and placed it in the `data` directory. Note that this dataset, on the whole, is structured nicely and has already undergone some cleaning. However, this is not the norm in real-life data science! We've chosen this dataset so that we can concentrate on learning and understanding the core concepts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f537df-169a-4a08-bced-f9687238654a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import seaborn as sns\n",
    "\n",
    "from pyprojroot import here\n",
    "from string import punctuation\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4190e351-97b7-4c5b-866e-07aa6cbd42c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import pandas\n",
    "# Use pandas to import tweets\n",
    "tweets_path = here('data/airline_tweets.csv')\n",
    "tweets = pd.read_csv(tweets_path, sep=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "880e8a36-bd58-4c24-8593-03a0ea70deed",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83108a6d-bfd9-4534-b628-6e77c8262821",
   "metadata": {},
   "source": [
    "Before we ever do any preprocessing or modeling, we always should do some exploratory data analysis to get a feel for the dataset.\n",
    "\n",
    "First, let's take a look at the first few rows and all the columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79acbaf2-6625-4abb-b50f-97ea54ba0d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80232c78-ac41-4d74-a581-76c9dac3b8f6",
   "metadata": {},
   "source": [
    "We have a `tweet_id`, which uniquely identifies each tweet. We also have an `airline_sentiment`, which takes on values of `\"positive\"`, `\"negative\"`, or `\"neutral\"`. There are other columns indicating the author of the tweet, when it was created, the timezone of the user, and others. The main column of interest is the `text` column: these are the tweets. Let's take a look at a few:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "438830e6-1064-47fe-b578-a1ca693a0ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in range(5):\n",
    "    print(tweets['text'].iloc[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d6746f8-b29c-40d4-bef6-b4afd4cd6cc1",
   "metadata": {},
   "source": [
    "We can already see that some of these tweets very obviously negative sentiment - how can you tell this is the case? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0400a77f-7e79-44bf-81dc-40bf8953a6d1",
   "metadata": {},
   "source": [
    "Let's take a look at which airlines are tweeted about and how many of each in this dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e98f355-830d-4d48-a1ad-16b1d8b29d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "airlines = tweets['airline']\n",
    "sns.countplot(x=airlines, order=airlines.value_counts().index)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c82c497c-4138-47eb-b959-235b552e7a47",
   "metadata": {},
   "source": [
    "## Challenge 1: Getting to Know the Data\n",
    "\n",
    "Use `pandas` to find out the following about the airline tweets:\n",
    "\n",
    "* How many tweets are in the dataset?\n",
    "* How many tweets are positive, neutral, and negative?\n",
    "* What *proportion* of tweets are positive, neutral, and negative?\n",
    "* Make a bar plot showing the proportion of tweet sentiments.\n",
    "\n",
    "If you have time, try the following:\n",
    "\n",
    "* How much time separates the earliest and latest tweets?\n",
    "* What gets more retweets: positive, negative, or neutral tweets?\n",
    "* Identify the airline whose tweets have the highest proportion of negative sentiment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e513930-2dc7-489c-bc5a-22eb09add5bf",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67bbe641-7d4a-4ab1-831b-a1f6ca15f3a5",
   "metadata": {},
   "source": [
    "We spent much of the last workshop learning how to preprocess the data. Let's apply what we learned to this dataset. Looking at some of the tweets above, we can see that while they are in pretty good shape, we can do some additional processing on them.\n",
    "\n",
    "In our pipeline, we'll omit the tokenization process, since we will perform it in a later step. Instead, we'll add a couple new preprocessing steps now that we are working with social media data. Specifically, we'll replace all hashtags with a \"HASHTAG\" token, and we'll replace all users (denoted by the \"@\" symbol) with a \"USER\" token. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a83ece-f3b2-4200-9d22-0788fbc07fa4",
   "metadata": {},
   "source": [
    "## Challenge 2: Creating a Preprocessing Pipeline for Social Media Data\n",
    "\n",
    "Write a function called `preprocess()` that performs the following on a text input:\n",
    "\n",
    "* Lowercase text.\n",
    "* Replace all URLs with the token \"URL\".\n",
    "* Replace all numbers with the token \"DIGIT\".\n",
    "* Replace hashtags with the token \"HASHTAG\".\n",
    "* Replace all users with the token \"USER\".\n",
    "* Remove blankspaces.\n",
    "\n",
    "We have provided regex patterns for each of the replacement steps in the following cells.\n",
    "\n",
    "Run your `preprocess()` function on `example_tweet` (two cells below), and when you think you have it working, apply it to the entire `text` column in the tweets DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb499df1-b1ce-41f8-a41b-3a3fbf51bbe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful regex patterns\n",
    "url_pattern = r'https?:\\/\\/.*[\\r\\n]*'\n",
    "digit_pattern = '\\d+'\n",
    "hashtag_pattern = r'(?:^|\\s)[ï¼ƒ#]{1}(\\w+)'\n",
    "user_pattern = r'@(\\w+)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75bb1fab-3ad6-486d-81f1-b294cf5e653a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply your function to the following example\n",
    "example_tweet = \"lol @justinbeiber and @BillGates are like soo 2000 #yesterday #amiright saw it on https://twitter.com #yolo\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33052a00-679c-4f78-b934-1634c144d202",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    # YOUR CODE HERE\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "728c43fb-13aa-4b1b-a99b-4a92f9404520",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on example tweet\n",
    "preprocess(example_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f7bb6a-f064-48cc-b650-12c4ef2fbb88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply to text column to create a new column\n",
    "tweets['text_processed'] = tweets['text'].apply(lambda x: preprocess(x))\n",
    "tweets['text_processed'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1576acc6-b305-492a-8fde-65b343cb779c",
   "metadata": {},
   "source": [
    "Preprocessing is complete - time for the bag-of-words!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53282330-54da-4e1c-bfe5-e77cb8fa3add",
   "metadata": {},
   "source": [
    "# The Bag-of-Words Representation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e55969dd-18a3-4ca3-a0f7-ad8f628bd5c5",
   "metadata": {},
   "source": [
    "The fundamental principle behind bag-of-words is to encode the corpus in terms of word frequencies. Consider the case of sentiment: we know sentiment is conveyed more strongly by specific words. For example, if a tweet contains the word \"happy\", it likely conveys positive sentiment (but not always - someone might say they're \"not happy\" - the opposite sentiment!). Furthermore, when those words come up more often, they'll probably more strongly convey the sentiment.\n",
    "\n",
    "In a bag-of-words, we taken some text, tokenize it, and then tabulate the frequencies of each token. The numerical representation of the text, then, is a vector indicating the frequencies of each token for that text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec7287d9-7307-4b5e-b0b3-ae4c7334b705",
   "metadata": {},
   "source": [
    "For example, if we're considering a movie review as follows: \n",
    "\n",
    "![bow](../images/bow.png)\n",
    "\n",
    "We take each token from the review, \"toss it in a bag\", and count up the frequencies of each word. The numerical representation, then, is the vector on the right: the number of appearances of each token. The \"bag\" here denotes that we are not modeling structure within the text - only the frequencies of the words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9d9bdbd-406d-469b-a8f6-41d1b3687c37",
   "metadata": {},
   "source": [
    "## Document Term Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33692097-d13e-4797-bb07-510c565b6cdc",
   "metadata": {},
   "source": [
    "In most text corpora, we will have many samples or *documents*. For example, in the airline tweets dataset, we have many tweets. Each tweet stands on its own as a unique sample: they can each be thought of as a unique document in the entire *corpus*. Since they are all related to each other, many tokens might be shared across tweets. So, when creating the bag-of-words model, we can tokenize across all documents, forming a *vocabulary*. Then, we can represent a single document by which of the tokens in the vocabulary are represented, and their frequency within the document.\n",
    "\n",
    "If the vocabulary has $V$ tokens, then each document will be encoded in a $V$-dimensional vector. If there are $D$ documents, the entire dataset can be represented in a $D \\times V$ matrix, where each row corresponds to the document, and each column corresponds to the token (or \"term\"). This $D \\times V$ matrix is a **document term matrix** (DTM).\n",
    "\n",
    "Let's consider a simple example. Suppose we have the \"documents\":\n",
    "\n",
    "```\n",
    "[\"You are at a workshop. Are you ready?\",\n",
    " \"Welcome to Berkeley!\",\n",
    " \"I am teaching a workshop.\"]\n",
    "```\n",
    "\n",
    "The unique (word) tokens in this \"corpus\", in alphabetical order, are:\n",
    "\n",
    "```\n",
    "[a, am, are, at, berkeley, i, ready, teaching, to, welcome, workshop, you]\n",
    "```\n",
    "\n",
    "The DTM can be formed by going through each document, ticking off the frequency of each token in each document, and plugging this number into the matrix:\n",
    "\n",
    "$$\n",
    "\\begin{array}{c|cccccccccccc}\n",
    " & \\text{a} & \\text{am} & \\text{are} & \\text{at} & \\text{berkeley} & \\text{i} & \\text{ready} & \\text{teaching} & \\text{to} & \\text{welcome} & \\text{workshop} & \\text{you} \\\\\\hline\n",
    "\\text{Document 1} & 1 & 0 & 2 & 1 & 0 & 0 & 1 & 0 & 0 & 0 & 1 & 1 \\\\\n",
    "\\text{Document 2} & 0 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 1 & 1 & 0 & 0 \\\\\n",
    "\\text{Document 3} & 1 & 1 & 0 & 0 & 0 & 1 & 0 & 1 & 0 & 0 & 1 & 0 \\\\\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "The numerical representation for each document is a row in the matrix. For example, Document 1 has numerical representation $[1, 0, 2, 1, 0, 0, 1, 0, 0, 0, 1, 1]$.\n",
    "\n",
    "To create a DTM, we will use the `CountVectorizer` from the package `sklearn`, a heavily used machine learning package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd2adf56-ba93-459d-8cfa-16ce8dc9284b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f41838-a387-43d0-b4f7-c2e0dba4d050",
   "metadata": {},
   "source": [
    "If you're not familiar with `sklearn`, here is the general workflow:\n",
    "\n",
    "1. We first create a `CountVectorizer` object, and choose specific settings for how we'll go about creating the DTM. Check out the [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) to see what options are available.\n",
    "2. Then, we \"fit\" this `CountVectorizer` object to the data. In this context, \"fitting\" consists of establishing a vocabulary of tokens from the documents in your dataset.\n",
    "3. Finally, we \"transform\" the data according to the \"fitted\" `CountVectorizer` object. This means taking our text data and transforming it into a DTM according to the vocabulary established by the \"fitting\" step.\n",
    "4. You can do steps 2 and 3 in one fell swoop using a `fit_transform` function.\n",
    "\n",
    "Let's start by creating a `CountVectorizer` with the default settings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "783e44a4-4a22-4290-b222-282b02c080dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f4f8d2f-c21e-40a8-9ad0-4a8f1a8fe005",
   "metadata": {},
   "source": [
    "Next, we'll fit *and* transform the airline tweets data. What does the documentation say about this function? \n",
    "\n",
    "We need to pass in all the tweets. What is returned?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f85e76ea-bc54-4775-bcda-432a03d2c96f",
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = vectorizer.fit_transform(tweets['text_processed'])\n",
    "counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8036ca73-2d0b-4781-9f2c-a215c838f4b2",
   "metadata": {},
   "source": [
    "It's not *quite* a `numpy` array. Instead, it's a `numpy` array stored in \"Compressed Sparse Format\". This format saves a lot of memory, but it is difficult to look at for a human. To illustrate the techniques in this lesson we will first uncompress this matrix. For larger datasets, however, you'll want to avoid this, as there are performance benefits to using CSF.\n",
    "\n",
    "Converting to a normal `numpy` array is easy: we use a built-in `todense()` function, and pass this into the `np.array()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87119057-c78c-4eb2-a9d6-3e9f44e4c22b",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(counts.todense())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b33def-90fe-4130-9a26-dbee653b9e1c",
   "metadata": {},
   "source": [
    "There are a lot of zeros here! This makes sense: there are probably a lot of tokens in the vocabulary, and each tweet likely only has a few of the tokens.\n",
    "\n",
    "It would be good to know which column refers to which tokens. Let's create a `pandas` DataFrame which has this information. First, we'll need to get the names of each token - how can we do this? Hint: always read the documentation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99322b85-1a15-46a5-bb80-bb5eaa6eeb7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract tokens\n",
    "tokens = vectorizer.get_feature_names_out()\n",
    "# Create DTM\n",
    "dtm = pd.DataFrame(data=counts.todense(),\n",
    "                   index=tweets.index,\n",
    "                   columns=tokens)\n",
    "# Look at DTM\n",
    "print(dtm.shape)\n",
    "dtm.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdeb9e39-93c0-4301-bf9d-c4243a12d96b",
   "metadata": {},
   "source": [
    "What can we do with the DTM? For one, we can see the most frequent tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f432154a-eae0-4723-a797-55f3cfdd71c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtm.sum().sort_values(ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec2e239-4bb8-4452-a358-cc10c46b8786",
   "metadata": {},
   "source": [
    "## Challenge 3: DTM Data Analysis\n",
    "\n",
    "* Print out the most infrequent words rather than the most frequent words. If you're not sure how, check the [documentation](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.sort_values.html)!\n",
    "* Print the average number of times each word is used in a tweet.\n",
    "* Which non-hashtag, non-digit token appears the most in any given tweet? How many times does it appear? What is the original tweet?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dba8e37-4880-4565-b6fc-7e7c96958f0f",
   "metadata": {},
   "source": [
    "## Customizing the `CountVectorizer`\n",
    "\n",
    "Recall the documentation on the `CountVectorizer` object: there are many options here on how you can customize the procedure of generating the vocabulary. Let's highlight some of these values:\n",
    "\n",
    "* `lowercase`: If `True`, it will lowercase all text. This was one of our preprocessing steps from before!\n",
    "* `stop_words`: You can choose or provide a list of stop-words. The `\"english\"` option is a built-in stop-word list, but you could provide another (e.g., the stop-word list provided by `nltk`).\n",
    "* `min_df` or `max_df`: The **document frequency** is the fraction of documents that a token appears in. If this value is high (close to 1), a token appears in most documents. If it's low (close to 0), it appears in very few documents. You might want to remove very rare tokens (they could be typos or gibberish) and you might want to remove very common tokens (these could be uninformative words that aren't stop words). These arguments allow you to specify a range of desired document frequencies. They can either be counts or fractions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37a0a93e-9dd8-43dc-a82c-06a24bf02bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the vectorizer\n",
    "vectorizer = CountVectorizer(\n",
    "    lowercase=True,\n",
    "    stop_words='english',\n",
    "    min_df=2,\n",
    "    max_df=0.95)\n",
    "# Fit, transform, and get tokens\n",
    "counts = vectorizer.fit_transform(tweets['text_processed'])\n",
    "tokens = vectorizer.get_feature_names_out()\n",
    "# Create dataframe\n",
    "dtm = pd.DataFrame(data=counts.todense(),\n",
    "                   index=tweets.index,\n",
    "                   columns=tokens)\n",
    "print(dtm.shape)\n",
    "dtm.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d1efefa-058b-4016-ba88-7e4e3aa65538",
   "metadata": {},
   "source": [
    "How did the number of tokens change with our new parameter settings?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e8b5145-d505-4e36-9a39-a40d25d8ec6f",
   "metadata": {},
   "source": [
    "## Challenge 4: Customizing the Vectorizer with `nltk` inputs\n",
    "\n",
    "If you look at the `CountVectorizer` documentation, you'll see that it can actually accept a custom `tokenizer` and `stop_words` list. \n",
    "\n",
    "Using what you learned in the previous workshop, create a `CountVectorizer` that utilizes the `nltk` word tokenizer and stop word list. How does the resulting DTM look different?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da610560-62c3-48ab-a1b2-25e0b589bc61",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38363398-fdf5-456b-ae3d-cae9d5294140",
   "metadata": {},
   "source": [
    "## Term Frequency-Inverse Document Frequency Scores (TF-IDF)\n",
    "\n",
    "So far, we're relying on word frequencies to give us information about a document. This assumes if a word appears more often in a document, it's more informative. However, this may not always be the case. For example, we already remove stop-words because they are not informative, despite the fact that they appear many times in a document. In the airline tweets, the word `flight` appears many times across the corpus, but is also not that informative, because it appears in many documents. Since we're looking at airline tweets, we shouldn't be surprised to see the word `flight`!\n",
    "\n",
    "To remedy this, we use a weighting scheme called **tf-idf (term frequency-inverse document frequency)**. The big idea behind tf-idf is to weight words not just by their frequency *within* a document, but by their frequency in one document *relative* to the remaining documents. Words that are frequent, but also used in every single document, will not be that informative. We want to identify words that are unevenly distributed across the corpus: these are the words most likely to be informative in a particular document.\n",
    "\n",
    "So, when we construct the DTM, we will be assigning each value a **tf-idf score**. Specifically, term $t$ in document $d$ is assigned tf-idf score as follows:\n",
    "\n",
    "$$\n",
    "\\text{tf-idf}(d, t) = \\text{tf}(d, t) \\times \\text{idf}(t)\n",
    "$$\n",
    "\n",
    "where $\\text{tf}(d, t)$ is the term-frequency of term $t$ in document $d$ and $\\text{idf}(t)$ is the inverse-document frequency of term $t$. \n",
    "\n",
    "When we used the `CountVectorizer` above, we only considered **term frequency**: if a term appeared more times in a document, it was given a higher value in the DTM. Now, we are scaling the term frequency by the **inverse document frequency**: if a term appears in many documents, we give it a lower weight, since it is less \"surprising\". \n",
    "\n",
    "We're not done yet: we still need to define the actual functions. In the `CountVectorizer`, the `tf(d, t)` function was simply $n_{d,t}$, or the number of times term $t$ appeared in document $d$. However, we can use other functions. In this case, we'll use:\n",
    "\n",
    "$$\n",
    "\\text{tf}(d, t) = \\frac{n_{d,t}}{N_d}\n",
    "$$\n",
    "\n",
    "where $N_d$ is the number of terms in document $d$. All we're doing here is normalizing the term frequency used in the `CountVectorizer`. Next, the inverse document frequency can be calculated as\n",
    "\n",
    "$$\n",
    "\\text{idf}(t) = 1 + \\log\\left(\\frac{D}{D_t}\\right)\n",
    "$$\n",
    "\n",
    "where $D$ is the total number of documents, and $D_t$ is the number of documents containing term $t$. If every document contains term $t$, then $\\text{idf}(t) = 1$, and no scaling will happen. If very few documents contain term $t$, then $\\text{idf}(t)$ will be very large, and the term frequency will be scaled up. In practice, the inverse document frequency is computed as \n",
    "\n",
    "$$\n",
    "\\text{idf}(t) = 1 + \\log\\left(\\frac{1 + D}{1 + D_t}\\right)\n",
    "$$\n",
    "\n",
    "to prevent any issues with zero occurrences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "769fe459-2aff-4304-af46-5c97779915db",
   "metadata": {},
   "source": [
    "We can also create a tf-idf DTM using `sklearn`. We'll use a `TfidfVectorizer` this time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e32d8a-c42d-475f-aab4-21eca8b1aee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d23916c1-5693-456c-b71d-6d9d78d1e2e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "dtm = vectorizer.fit_transform(tweets['text_processed'])\n",
    "dtm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e509c8-5402-4be0-9143-0e448fff7066",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = pd.DataFrame(dtm.todense(),\n",
    "                     columns=vectorizer.get_feature_names_out(),\n",
    "                     index=tweets.index)\n",
    "tfidf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "877eb33e-f7dd-4e49-8d2f-eb5d14a36297",
   "metadata": {},
   "source": [
    "Let's look at the 20 words with the highest tf-idf weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b0ced2b-22b2-4ee5-89b2-2fbba93e10e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf.max().sort_values(ascending=True).head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da410cb3-a452-441b-a94d-8f751d59d7a6",
   "metadata": {},
   "source": [
    "# Supervised Learning: Sentiment Classification\n",
    "\n",
    "Now that we have a numerical representation of the text, we'd like to *do* something with it. A common task is supervised learning: using the numerical representation to predict some kind of label about the text. Text classification can consist of many types of analysis, such as:\n",
    "\n",
    "* Sentiment analysis\n",
    "* Genre classification\n",
    "* Language identification\n",
    "* Authorship attribution\n",
    "* Spam detection\n",
    "* Document relevancy\n",
    "\n",
    "and many others. How exactly do we go about doing this?\n",
    "\n",
    "Let's consider a toy example:\n",
    "\n",
    "```\"This was the best service! Would love to come again!\"```\n",
    "\n",
    "This is very clearly expressing positive sentiment. But how did we make that judgement?\n",
    "\n",
    "* The review claims the service is the \"best\". This is pretty positive.\n",
    "* The reviewer says they would \"love\" to come again. This is also positive.\n",
    "\n",
    "Specific key words (tokens) were predictive of the sentiment. If someone says something was the \"best\", it's a good sign there's positive sentiment in the text. The semantic meaning of these words helps convey sentiment. \n",
    "\n",
    "This is how we can proceed with classification. We'll construct a DTM from the text data, and use that to predict the labels using a machine learning model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1121954-e13c-4bdc-9323-6834060d7197",
   "metadata": {},
   "source": [
    "## Predicting Sentiment with Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df3ab3a9-ce76-4daf-84c0-5f65bd32904b",
   "metadata": {},
   "source": [
    "We're going to use a logistic regression model to predict the labels. If you're not familiar with this model, or the basics of machine learning, we recommend taking the Python Machine Learning workshop offered at the D-Lab. For now, we'll review some basics.\n",
    "\n",
    "The DTM is a $D\\times T$ matrix, where $D$ is the number of documents, and $T$ is the number of terms. We can think of the $T$ terms as the \"features\": these values are what we'll use to predict the sentiment. The $D$ documents represent samples: we'll use the patterns across these samples to learn a relationship between the feature and the outcome.\n",
    "\n",
    "In logistic regression, we are learning a **linear model** represented by specific parameters, which we'll call $\\beta_i$. For features $x_i$ (this is a row in the DTM), we construct a **logit**:\n",
    "\n",
    "$$\n",
    "L = \\beta_1 x_1 + \\beta_2 x_2 + \\cdots + \\beta_T x_T\n",
    "$$\n",
    "\n",
    "and obtain a probability $p$ by passing this logit through the **sigmoid function**, which maps any value onto the range $[0, 1]$:\n",
    "\n",
    "$$\n",
    "p = \\text{sigmoid}(L) = \\frac{1}{1 + \\exp(-L)}\n",
    "$$\n",
    "\n",
    "We can quite literally think of $p$ as a probability that the sample falls in one of the two classes. If, say, $p>0.5$, we call the sample positive sentiment; otherwise, it's negative sentiment.\n",
    "\n",
    "So, to summarize: we take each sample, which consists of features $x = (x_1, x_2, \\ldots, x_T)$, multiply them by $\\beta_i$ values, and add them up. Pass them through a sigmoid, and get a probability. The key question is: how do we know what $\\beta_i$ values to use? We won't discuss thus details here, but an optimization algorithm will learn the best values given the data. \n",
    "\n",
    "Let's train a model! We're going to use `sklearn` to do this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33413d63-87eb-489f-b374-3cfeaa51cf3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "330d33c6-2640-44cd-8a3a-3dd62b98f3af",
   "metadata": {},
   "source": [
    "To understand the theoretical gist of our classification task, let's first focus on a binary 'positive vs negative' classifier. We are going to do so by restricting the analysis to the non-neutral tweets. So, we'll first partition the dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "508769ec-dd07-4492-8308-17e031a522da",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_binary = tweets[tweets['airline_sentiment'] != 'neutral']\n",
    "tweets_binary.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ca64c2-0cfe-4e12-ae44-74c75fae456d",
   "metadata": {},
   "source": [
    "Next, we're going to apply a tf-idf vectorizer to the data. We're going to make use of the `max_features` argument to restrict the number of tokens our model has to consider:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9358ed4d-8108-4fef-9a97-0271544bd30f",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(max_features=5000)\n",
    "dtm = vectorizer.fit_transform(tweets_binary['text_processed'])\n",
    "X = np.asarray(dtm.todense())\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44a6a8e5-da30-4363-8f17-112a0fe94083",
   "metadata": {},
   "source": [
    "Now, we need to get the labels for each of the 11,541 tweets. We can do this by simply extracting the `airline_sentiment` column - `sklearn` is smart enough to handle the details later. Let's also check out the distribution of the labels, to get a sense for a good baseline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e7df4f3-0fb0-406f-9240-c9f9761eabea",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = tweets_binary['airline_sentiment']\n",
    "print(y.value_counts())\n",
    "print(y.value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee87ff74-3fbb-472a-b795-6f4d18fab215",
   "metadata": {},
   "source": [
    "Machine learning fundamentally requires that we learn the $\\beta_i$ values from a **training set**, and then evaluate the performance on a separately held **test set**. This ensures that we actually develop an algorithm that can **generalize**. We'll use the `train_test_split` function from `sklearn` to separate our data into two sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64cec8b9-14d9-4897-9c02-cc89fcf7b3c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "066771d8-2f31-4646-9a1b-6d2b1b9b208c",
   "metadata": {},
   "source": [
    "In order to streamline the training process, we've written a `fit_logistic_regression` function you can use to easily train a model given the data inputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d46de0b2-af00-4a1d-b4cd-31b96ce545d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_logistic_regression(X, y):\n",
    "    \"\"\"Fits a logistic regression model to provided data.\"\"\"\n",
    "    model = LogisticRegressionCV(\n",
    "        Cs=10,\n",
    "        penalty='l1',\n",
    "        cv=3,\n",
    "        solver='liblinear',\n",
    "        refit=True).fit(X, y)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "124aa7ea-1bc1-43e2-beeb-0ba2da9b2df9",
   "metadata": {},
   "source": [
    "We'll fit the model, and see how it performs on both the training and test sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "773963bd-6603-4fad-884b-09ce60afab18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the logistic regression model\n",
    "model = fit_logistic_regression(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3abaa8c-cbb9-46cf-925a-0248a638f667",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Training accuracy: {model.score(X_train, y_train)}\")\n",
    "print(f\"Test accuracy: {model.score(X_test, y_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e186c5-1719-4deb-bdb4-614a9980f058",
   "metadata": {},
   "source": [
    "The model got ~95% accuracy on the training set, and ~92% on the test set - that's pretty good! The similarity between the two performances is also a good sign - it means we were able to generalize pretty well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c11b5232-b9f5-471b-8269-ee529a6b1072",
   "metadata": {},
   "source": [
    "## Validating Model Performance on New Tweets\n",
    "\n",
    "Now that we have a trained model, there's nothing stopping us from using it on new data! The `model` object comes equipped with a `predict` function that we can use to evaluate on new text samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c7ea47-4d82-4918-9875-d0520358de6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some new tweets\n",
    "new_tweets = [example_tweet,\n",
    "              'omg I am never flying on United again',\n",
    "              'I love @VirginAmerica so much #friendlystaff',\n",
    "              'food on Air France is great!']\n",
    "# First, we need to preprocess them\n",
    "new_tweets_processed = [preprocess(tweet) for tweet in new_tweets]\n",
    "# Next, we need to vectorize them\n",
    "X_new = np.asarray(vectorizer.transform(new_tweets_processed).todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf110a09-e99a-40fa-adef-51b1482da413",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run predictions\n",
    "model.predict(X_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f31f1f-1eeb-4811-819a-a6ccba08fe59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get probabilities\n",
    "model.predict_proba(X_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88ec2ccf-e339-4f08-bc86-9d2aa42864d6",
   "metadata": {},
   "source": [
    "Those predictions look pretty good! Feel free to give the model a try on other tweets you write."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "310dac39-4753-4ae8-8dfa-e65e5824cccb",
   "metadata": {},
   "source": [
    "## Interpreting the Model Coefficients"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5a48bd9-68d0-45fe-9491-6ffcec79eae9",
   "metadata": {},
   "source": [
    "The nice thing about logistic regression is that it is **interpretable**. Take a look at the logit again:\n",
    "\n",
    "$$\n",
    "L = \\beta_1 x_1 + \\beta_2 x_2 + \\cdots + \\beta_T x_T\n",
    "$$\n",
    "\n",
    "If $L$ is very positive, the sigmoid will make sure that the probability $p$ is close to 1 (positive sentiment). If $L$ is very negative, the sigmoid will make sure that the probability $p$ is close to 0 (negative sentiment). So, by looking at the coefficients $\\beta_i$, we can see how each feature (token) impacts the eventual prediction. If $\\beta_i >0$, then the presence of that feature implies positive sentiment, according to the model. If  $\\beta_i < 0$, then the presence of that feature implies negative sentiment, according to the model.\n",
    "\n",
    "So, let's take a look at the fitted coefficients to see if what we see makes sense! We can access them using the `coef_` member, and we can match each coefficient to the tokens from the vectorizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dcb6ef1-13b3-437e-813c-7118911847a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "coefs = model.coef_.ravel()\n",
    "tokens = vectorizer.get_feature_names_out()\n",
    "importance = pd.DataFrame()\n",
    "importance['token'] = tokens\n",
    "importance['coefs'] = coefs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5598e78-924d-41a5-a442-dc9c32ce2df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "importance.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e63814e-9c0d-4f7a-a5e0-72cca2758d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "importance.sort_values('coefs')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eed48ea-fd35-4585-9b98-90456aaee447",
   "metadata": {},
   "source": [
    "This makes sense! Tokens like \"worst\", \"not\", and \"rude\" imply negative sentiment, while tokens like \"thanks\", \"kudos\", \"awesome\" imply positive sentiment. Interestingly, the token \"worries\" implies positive sentiment - what does this tell you about how people typically use this token in their tweets?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7803473d-9544-4e6d-a840-241cd9f810f6",
   "metadata": {},
   "source": [
    "## Challenge 5\n",
    "\n",
    "Try developing a **multinomial logistic regression** model, to predict positive, negative, and neutral labels. We've provided you a fitter function below, but it's up to you to create new labels, train-test splits, and perform the fitting and evaluation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17933ad2-ddd2-42ab-bebb-d5a344848211",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_multinomial_logistic_regression(X, y):\n",
    "    \"\"\"Fits a logistic regression model to provided data.\"\"\"\n",
    "    model = LogisticRegressionCV(\n",
    "        multi_class='multinomial',\n",
    "        Cs=10,\n",
    "        penalty='l1',\n",
    "        cv=3,\n",
    "        solver='saga',\n",
    "        refit=True).fit(X, y)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5efe867-1e98-4e82-8b55-6251061321e5",
   "metadata": {},
   "source": [
    "## Challenge 6\n",
    "\n",
    "Create a new fitter function that uses a `RandomForestClassifier`. How is the performance? Check the [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf07d41-bcf1-4ed2-a699-a53f26a5ff2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_random_forest(X, y):\n",
    "    \"\"\"Fits a random forest model to provided data.\"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    return"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
