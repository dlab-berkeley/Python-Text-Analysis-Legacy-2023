{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c0c2f15-1a64-4b2b-8ca1-270f05aa24b9",
   "metadata": {},
   "source": [
    "# Introduction to Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edf6a718-71e0-49d9-a1ed-7b0ab99d4e81",
   "metadata": {},
   "source": [
    "Thus far, we've focused on bag-of-word approaches to text analysis, where the text is represented as a vector of word frequencies. This generally works pretty well - we can do a decent job of topic modeling and supervised classification with this approach. However, word frequencies alone don't tell the whole picture. The ordering of words, for example, provides additional context that word frequencies don't capture. Furthermore, words can be used in a variety of ways, with different meanings that get lost in a word frequency representation.\n",
    "\n",
    "An alternative formalization of text consists of representing the words (or bi-grams, phrases, etc.) as vectors. They're also called word embeddings, because we embed the word in a higher dimensional space. A word vector has no inherent meaning to humans - ultimately, it's just a bunch of floating point numbers. But word vectors are useful because they're a numerical representation of text that captures its semantic meaning, and can easily be used in downstream tasks, such as dictionary methods, classification, topic modeling etc. Furthermore, the vector representation can be used to perform semantic tasks, such as finding synonyms, testing analogies, and others. The million dollar question, however, is: how do we create the word vector in the first place?\n",
    "\n",
    "The answer to the million dollar question is to pick the right task. Specifically, we're going to calculate the word vectors so that they can be successfully used in one of two tasks: predicting surrounding words, or predicting words within a context. In this workshop, we're going to both use pre-trained word embeddings, and construct our own word embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94fc6996",
   "metadata": {},
   "source": [
    "# The Word Embedding Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c70bc214",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3aaa0a94",
   "metadata": {},
   "source": [
    "# Install and Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b7b49d-86ab-45d3-9e63-42926c134b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff20430",
   "metadata": {},
   "source": [
    "We'll be using a package called `gensim` to conduct our word embedding experiments. `gensim` is one of the major Python packages for natural language processing, largely aimed at using different kinds of embeddings.\n",
    "\n",
    "If you don't have `gensim` installed, you can install it directly within this notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d66811",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run if you do not have gensim installed\n",
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b84a258",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f162cd-e284-4081-bf8f-e052e492e9e8",
   "metadata": {},
   "source": [
    "# Using Pre-trained Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e01cd2bc",
   "metadata": {},
   "source": [
    "The first thing we'll do is use a pre-trained word embedding. This means that we're downloading a word embedding model that has already been trained on a large corpus. Researchers have trained a variety of models in different contexts that are freely available on `gensim`. We can take a look at a few of them by looking in the `gensim` downloader:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9042101c",
   "metadata": {},
   "outputs": [],
   "source": [
    "gensim_models = list(api.info()['models'].keys())\n",
    "print(gensim_models)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b25caa4",
   "metadata": {},
   "source": [
    "We are going to use the `word2vec-google-news-300` model: this is a word embedding model that is trained on Google News, where the embedding is 300 dimensions. Downloading this might take a while! The word embedding model is nearly 2 GB. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28406e56-9728-40df-a370-aafb9cce12d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "wv = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4455f5da",
   "metadata": {},
   "source": [
    "How many word vectors are available in this word embedding model? We can access the `index_to_key` member variable to find out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41831a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_words = len(wv.index_to_key)\n",
    "print(f\"Number of words: {n_words}\")\n",
    "print(wv.index_to_key[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ac1e2ab",
   "metadata": {},
   "source": [
    "The model is trained using a vocabulary of size 3 million! This is a huge model, which takes hours to train. This is why we used a pre-trained model - we likely don't have the resources to train this on our local machines.\n",
    "\n",
    "Accessing the actual word vectors can be done by treating the word vector model as a dictionary. For example, let's take a look at the word vector for `\"banana\"`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c160fd83",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(wv[\"banana\"])\n",
    "print(wv[\"banana\"].size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f75bb9eb",
   "metadata": {},
   "source": [
    "As promised, the word vector is 300-dimensional. Looking at the actual values of the vector is pretty uninformative - the values appear to be random floats. However, now that the word has been transformed into a vector, we can more easily perform computations on it that correspond to semantic operations. Let's take a look at a few examples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47060f92",
   "metadata": {},
   "source": [
    "## Word Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7673852e",
   "metadata": {},
   "source": [
    "A semantic question we can ask is  that are similar to \"banana\". How does word similarity look in vector operations? We'd expect similar words to have vectors that are closer to each other in vector space.\n",
    "\n",
    "There are many metrics of vector similarity - one of the most useful ones is the [cosine similarity](https://en.wikipedia.org/wiki/Cosine_similarity). It has a range of 0 to 1, with orthogonal vectors have a cosine similarity of 0, and parallel vectors having a cosine similarity of 1. `gensim` provides a function that lets us find the most similar vectors to a queried vector - let's give it a shot! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ed3da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "wv.most_similar('banana')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "869df615",
   "metadata": {},
   "source": [
    "The most similar vectors to \"banana\" are other fruits and foods! These are conceptual relationships that are reflected in the word embedding that we did not explicitly train in the model. Let's try another, more abstract word:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd0e108-ebd2-4864-b436-7ff9f717cbf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "wv.most_similar('happy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da4a9a34",
   "metadata": {},
   "source": [
    "We see synonyms of \"happy\", and even an antonym (\"disappointed\"). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d095a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "wv.most_similar_cosmul()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b49ce27b",
   "metadata": {},
   "source": [
    "### Challenge 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee0ead0",
   "metadata": {},
   "source": [
    "Look up the `doesnt_match` function in `gensim`'s documentation. Use this function to identify which word doesn't match in the following group:\n",
    "\n",
    "banana, apple, strawberry, happy\n",
    "\n",
    "Then, try it on groups of words that you choose. Here are some suggestions:\n",
    "\n",
    "1. A group of fruits, and a vegetable. Can it identify that the vegetable doesn't match?\n",
    "2. A group of vehicles that travel by land, and a vehicle that travels by air (e.g., a plane or helicopter). Can it identify the vehicle that flies?\n",
    "3. A group of scientists (e.g., biologist, physicist, chemist, etc.) and a person who does not study science (e.g., an artist). Can it identify the occupation that is not science based?\n",
    "\n",
    "To be clear, `word2vec` does not learn the precise nature of the differences between these groups. However, the semantic differences correspond to similar words appearing near each other in large corpora."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "779b2d08-83b7-49ed-a917-97e10e34221e",
   "metadata": {},
   "source": [
    "## Word Analogies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a5bb85",
   "metadata": {},
   "source": [
    "One of the most famous usages of `word2vec` is via word analogies. For example:\n",
    "\n",
    "Paris is to France as Berlin is to Germany. \n",
    "\n",
    "Here, the analogy is between (Paris, France) and (Berlin, Germany), with \"capital city\" being the concept that connects them. We can abstract the \"analogy\" relationship to vector modeling. Let's pretend we're working with each of the vectors. Then, the analogy is\n",
    "\n",
    "$\\mathbf{v}_{\\text{France}} - \\mathbf{v}_{\\text{Paris}} \\approx \\mathbf{v}_{\\text{Germany}} - \\mathbf{v}_{\\text{Berlin}}.$\n",
    "\n",
    "The vector difference here represents the notion of \"capital city\". Presumably, going from the Paris vector to the France vector (i.e., the vector difference) will be the same as going from the Berlin vector to the Germany vector, if that difference carries similar semantic meaning.\n",
    "\n",
    "Let's test this directly. We'll do so by rewriting the above expression:\n",
    "\n",
    "$\\mathbf{v}_{\\text{France}} - \\mathbf{v}_{\\text{Paris}} + \\mathbf{v}_{\\text{Berlin}} \\approx \\mathbf{v}_{\\text{Germany}}.$\n",
    "\n",
    "We'll calculate the difference between Paris and France, add on Germany, and find the closest vector to that quantity. Notice that, in all these operations, we set `norm=True`, and renormalize. That's because different vectors might be of different lengths, so the normalization puts everything on a common scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5556ad23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate \"capital city\" vector difference\n",
    "difference = wv.get_vector('France', norm=True) - wv.get_vector('Paris', norm=True) \n",
    "# Add on Berlin\n",
    "difference += wv.get_vector('Berlin', norm=True)\n",
    "# Renormalize vector\n",
    "difference /= np.linalg.norm(difference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a43aa6cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What is the most similar vector?\n",
    "wv.most_similar(difference)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e9ecfe0",
   "metadata": {},
   "source": [
    "Germany is the most similar! So, word analogies seem possible with `word2vec`.\n",
    "\n",
    "Carrying out these operations can be done in one fell swoop with the `most_similar` function. Check the documentation for this function. What do the `positive` and `negative` arguments mean?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2b1de6f",
   "metadata": {},
   "source": [
    "## Challenge 2\n",
    "\n",
    "Carry out the following word analogies:\n",
    "\n",
    "1. Mouse : Mice :: Goose : ?\n",
    "2. Kangaroo : Joey :: Cat : ?\n",
    "3. Mexico : Peso :: Dollar : ?\n",
    "4. Happy : Sad :: Up : ?\n",
    "4. California : Sacramento :: Canada : ?\n",
    "5. California : Sacramento :: Washington : ?\n",
    "\n",
    "Some work well, and others don't work as well. Try to come up with your own analogies!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a3bef04-6c7a-4332-9a5d-ce4be72bd52c",
   "metadata": {},
   "source": [
    "# Creating Custom Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a87b1931",
   "metadata": {},
   "source": [
    "# Document Embeddings"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
