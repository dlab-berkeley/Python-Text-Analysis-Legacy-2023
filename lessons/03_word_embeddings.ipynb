{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c0c2f15-1a64-4b2b-8ca1-270f05aa24b9",
   "metadata": {},
   "source": [
    "# Python Text Analysis: Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edf6a718-71e0-49d9-a1ed-7b0ab99d4e81",
   "metadata": {},
   "source": [
    "Thus far, we've focused on bag-of-word approaches to text analysis, where the text is represented as a vector of word frequencies. This generally works pretty well - we can do a decent job of supervised classification with this approach. However, word frequencies alone don't tell the whole picture. The ordering of words, for example, provides additional context that word frequencies don't capture. Furthermore, words can be used in a variety of ways, with different meanings that get lost in a word frequency representation.\n",
    "\n",
    "An alternative formalization of text consists of representing the words (or bi-grams, phrases, etc.) as vectors. They're also called word embeddings, because we embed the word in a higher dimensional space. A word vector has no inherent meaning to humans - ultimately, it's just a bunch of floating point numbers. But word vectors are useful because they're a numerical representation of text that captures its semantic meaning, and can easily be used in downstream tasks, such as dictionary methods, classification, topic modeling etc. Furthermore, the vector representation can be used to perform semantic tasks, such as finding synonyms, testing analogies, and others. The big question, however, is: how do we create the word vector in the first place?\n",
    "\n",
    "The answer is to pick the right task. Specifically, we're going to calculate the word vectors so that they can be successfully used in one of two tasks: predicting surrounding words, or predicting words within a context."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94fc6996",
   "metadata": {},
   "source": [
    "# The Word Embedding Model: `word2vec`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef6396f1-6baa-49cb-8a4a-3679732f315e",
   "metadata": {},
   "source": [
    "The word embedding model, generally referred to as `word2vec`, was developed by [Mikolov et al.](https://arxiv.org/abs/1310.4546) in 2013. The basic premise is to find vector representations of tokens that have semantic meaning. How do we go about learning a \"good\" vector representation from the data?\n",
    "\n",
    "Mikolov et al. proposed two approaches: the **skip-gram (SG)** and the **continuous bag-of-words (CBOW)**. Both approaches are similar in that we use the vector representation of a token to try and predict what the nearby tokens are with a shallow neural network.\n",
    "\n",
    "![word2vec](../images/word2vec.png)\n",
    "\n",
    "In the continuous bag-of-words model, our goal is to predict a word $w(t)$, given the words that surround it - e.g., $w(t-2), w(t-1), w(t+1), w(t+2)$, etc. So, in an example text such as `I went to the store to get some apples`, we may try to use the word vectors for `I`, `went`, `to`, `the`, `to`, `get`, `some`, `apples` to predict the word `store`. This would correspond to a *window size* of 4: 4 words on either side of the target word.\n",
    "\n",
    "In the skip-gram model, we construct a word vector that can be used to predict the words surrounding a specific word $w(t)$. This is the reverse of the continuous bag-of-words, and is a harder task, since we have to predict more from less information. In the above example, we'd aim to predict the remaining words in the sentence from the word vector for `store`. \n",
    "\n",
    "You can use either approaches to build a set of word embeddings. Mikolov et al. demonstrated that the skip-gram works pretty well in larger corpuses. Furthermore, it's easier to train the skip-gram efficiently, making it faster.\n",
    "\n",
    "The mechanics of how the training is actually done revolves around a **shallow neural network**. An **objective function** is specified - a mathematical expression that quantifies how well we predicted a word - which allows the values of the word vectors to be optimized using **back propagation**. We won't go into these details for this workshop, but check out the Python Deep Learning workshop if you'd like to learn more about neural networks!\n",
    "\n",
    "Let's jump into it!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aaa0a94",
   "metadata": {},
   "source": [
    "# Installing `gensim`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b7b49d-86ab-45d3-9e63-42926c134b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegressionCV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff20430",
   "metadata": {},
   "source": [
    "We'll be using a package called `gensim` to conduct our word embedding experiments. `gensim` is one of the major Python packages for natural language processing, largely aimed at using different kinds of embeddings.\n",
    "\n",
    "If you don't have `gensim` installed, you can install it directly within this notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d66811",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run if you do not have gensim installed\n",
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b84a258",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import gensim.downloader as api"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f162cd-e284-4081-bf8f-e052e492e9e8",
   "metadata": {},
   "source": [
    "# Using Pre-trained Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e01cd2bc",
   "metadata": {},
   "source": [
    "The first thing we'll do is use a pre-trained word embedding. This means that we're downloading a word embedding model that has already been trained on a large corpus. Researchers have trained a variety of models in different contexts that are freely available on `gensim`. We can take a look at a few of them by looking in the `gensim` downloader:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9042101c",
   "metadata": {},
   "outputs": [],
   "source": [
    "gensim_models = list(api.info()['models'].keys())\n",
    "print(gensim_models)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b25caa4",
   "metadata": {},
   "source": [
    "We are going to use the `word2vec-google-news-300` model: this is a word embedding model that is trained on Google News, where the embedding is 300 dimensions. Downloading this might take a while! The word embedding model is nearly 2 GB. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28406e56-9728-40df-a370-aafb9cce12d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "wv = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4455f5da",
   "metadata": {},
   "source": [
    "How many word vectors are available in this word embedding model? We can access the `index_to_key` member variable to find out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41831a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_words = len(wv.index_to_key)\n",
    "print(f\"Number of words: {n_words}\")\n",
    "print(wv.index_to_key[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ac1e2ab",
   "metadata": {},
   "source": [
    "The model is trained using a vocabulary of size 3 million! This is a huge model, which takes hours to train. This is why we used a pre-trained model - we likely don't have the resources to train this on our local machines.\n",
    "\n",
    "Accessing the actual word vectors can be done by treating the word vector model as a dictionary. For example, let's take a look at the word vector for `\"banana\"`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c160fd83",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(wv[\"banana\"])\n",
    "print(wv[\"banana\"].size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f75bb9eb",
   "metadata": {},
   "source": [
    "As promised, the word vector is 300-dimensional. Looking at the actual values of the vector is pretty uninformative - the values appear to be random floats. However, now that the word has been transformed into a vector, we can more easily perform computations on it that correspond to semantic operations. Let's take a look at a few examples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47060f92",
   "metadata": {},
   "source": [
    "## Word Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7673852e",
   "metadata": {},
   "source": [
    "A semantic question we can ask is  that are similar to \"banana\". How does word similarity look in vector operations? We'd expect similar words to have vectors that are closer to each other in vector space.\n",
    "\n",
    "There are many metrics of vector similarity - one of the most useful ones is the [cosine similarity](https://en.wikipedia.org/wiki/Cosine_similarity). It has a range of 0 to 1, with orthogonal vectors have a cosine similarity of 0, and parallel vectors having a cosine similarity of 1. `gensim` provides a function that lets us find the most similar vectors to a queried vector - let's give it a shot! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ed3da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "wv.most_similar('banana')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "869df615",
   "metadata": {},
   "source": [
    "The most similar vectors to \"banana\" are other fruits and foods! These are conceptual relationships that are reflected in the word embedding that we did not explicitly train in the model. Let's try another, more abstract word:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd0e108-ebd2-4864-b436-7ff9f717cbf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "wv.most_similar('happy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da4a9a34",
   "metadata": {},
   "source": [
    "We see synonyms of \"happy\", and even an antonym (\"disappointed\"). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b49ce27b",
   "metadata": {},
   "source": [
    "## Challenge 1\n",
    "\n",
    "Look up the `doesnt_match` function in `gensim`'s documentation. Use this function to identify which word doesn't match in the following group:\n",
    "\n",
    "banana, apple, strawberry, happy\n",
    "\n",
    "Then, try it on groups of words that you choose. Here are some suggestions:\n",
    "\n",
    "1. A group of fruits, and a vegetable. Can it identify that the vegetable doesn't match?\n",
    "2. A group of vehicles that travel by land, and a vehicle that travels by air (e.g., a plane or helicopter). Can it identify the vehicle that flies?\n",
    "3. A group of scientists (e.g., biologist, physicist, chemist, etc.) and a person who does not study an empirical science (e.g., an artist). Can it identify the occupation that is not science based?\n",
    "\n",
    "To be clear, `word2vec` does not learn the precise nature of the differences between these groups. However, the semantic differences correspond to similar words appearing near each other in large corpora."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "779b2d08-83b7-49ed-a917-97e10e34221e",
   "metadata": {},
   "source": [
    "## Word Analogies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a5bb85",
   "metadata": {},
   "source": [
    "One of the most famous usages of `word2vec` is via word analogies. For example:\n",
    "\n",
    "`Paris : France :: Berlin : Germany`\n",
    "\n",
    "Here, the analogy is between (Paris, France) and (Berlin, Germany), with \"capital city\" being the concept that connects them. We can abstract the \"analogy\" relationship to vector modeling. Let's pretend we're working with each of the vectors. Then, the analogy is\n",
    "\n",
    "$\\mathbf{v}_{\\text{France}} - \\mathbf{v}_{\\text{Paris}} \\approx \\mathbf{v}_{\\text{Germany}} - \\mathbf{v}_{\\text{Berlin}}.$\n",
    "\n",
    "The vector difference here represents the notion of \"capital city\". Presumably, going from the Paris vector to the France vector (i.e., the vector difference) will be the same as going from the Berlin vector to the Germany vector, if that difference carries similar semantic meaning.\n",
    "\n",
    "Let's test this directly. We'll do so by rewriting the above expression:\n",
    "\n",
    "$\\mathbf{v}_{\\text{France}} - \\mathbf{v}_{\\text{Paris}} + \\mathbf{v}_{\\text{Berlin}} \\approx \\mathbf{v}_{\\text{Germany}}.$\n",
    "\n",
    "We'll calculate the difference between Paris and France, add on Germany, and find the closest vector to that quantity. Notice that, in all these operations, we set `norm=True`, and renormalize. That's because different vectors might be of different lengths, so the normalization puts everything on a common scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5556ad23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate \"capital city\" vector difference\n",
    "difference = wv.get_vector('France', norm=True) - wv.get_vector('Paris', norm=True) \n",
    "# Add on Berlin\n",
    "difference += wv.get_vector('Berlin', norm=True)\n",
    "# Renormalize vector\n",
    "difference /= np.linalg.norm(difference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a43aa6cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What is the most similar vector?\n",
    "wv.most_similar(difference)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e9ecfe0",
   "metadata": {},
   "source": [
    "Germany is the most similar! So, word analogies seem possible with `word2vec`.\n",
    "\n",
    "Carrying out these operations can be done in one fell swoop with the `most_similar` function. Check the documentation for this function. What do the `positive` and `negative` arguments mean?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2b1de6f",
   "metadata": {},
   "source": [
    "## Challenge 2\n",
    "\n",
    "Carry out the following word analogies:\n",
    "\n",
    "1. Mouse : Mice :: Goose : ?\n",
    "2. Kangaroo : Joey :: Cat : ?\n",
    "3. United States : Dollar :: Mexico : ?\n",
    "4. Happy : Sad :: Up : ?\n",
    "5. California : Sacramento :: Canada : ?\n",
    "6. California : Sacramento :: Washington : ?\n",
    "\n",
    "What about something more abstract, such as:\n",
    "\n",
    "7. United States : hamburger :: Canada : ?\n",
    "\n",
    "Some work well, and others don't work as well. Try to come up with your own analogies!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a3bef04-6c7a-4332-9a5d-ce4be72bd52c",
   "metadata": {},
   "source": [
    "# Creating Custom Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5397cb46-3c81-49e9-b7bb-37efe55803d4",
   "metadata": {},
   "source": [
    "In the previous example, we used a *pretrained* word embedding. That is, the word embedding was already trained using a very large corpus from Google News. What about when we want to train our own word embeddings from a new corpus?\n",
    "\n",
    "We can do that using `gensim` as well. However, if the corpus is large, training becomes very computationally taxing. So, we'll try training our own word embeddings, but on a much smaller corpus. Specifically, we'll return to one you should recognize: the airline tweets corpus!\n",
    "\n",
    "Let's go ahead and get set up by importing the dataset and preprocessing, as we did in Part 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dda1c4d-5006-422d-a3c6-857d59a08f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_path = '../data/airline_tweets.csv'\n",
    "tweets = pd.read_csv(tweets_path, sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a22b90-c66d-4ab0-a164-bd30c67241af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    \"\"\"Preprocesses a string.\"\"\"\n",
    "    # Lowercase\n",
    "    text = text.lower()\n",
    "    # Replace URLs\n",
    "    url_pattern = r'https?:\\/\\/.*[\\r\\n]*'\n",
    "    url_repl = ' URL '\n",
    "    text = re.sub(url_pattern, url_repl, text)\n",
    "    # Replace digits\n",
    "    digit_pattern = '\\d+'\n",
    "    digit_repl = ' DIGIT '\n",
    "    text = re.sub(digit_pattern, digit_repl, text)\n",
    "    # Replace hashtags\n",
    "    hashtag_pattern = r'(?:^|\\s)[＃#]{1}(\\w+)'\n",
    "    hashtag_repl = ' HASHTAG '\n",
    "    text = re.sub(hashtag_pattern, hashtag_repl, text)\n",
    "    # Replace users\n",
    "    user_pattern = r'@(\\w+)'\n",
    "    user_repl = ' USER '\n",
    "    text = re.sub(user_pattern, user_repl, text)\n",
    "    # Remove blank spaces\n",
    "    blankspace_pattern = r'\\s+'\n",
    "    blankspace_repl = ' '\n",
    "    text = re.sub(blankspace_pattern, blankspace_repl, text).strip()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb5e805-3750-449c-ac6e-6fee95c0896c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets['text_processed'] = tweets['text'].apply(lambda x: preprocess(x))\n",
    "tweets['text_processed'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb98145-a9ae-47ec-9928-99d850e57586",
   "metadata": {},
   "source": [
    "To create our own model, we need to import the `Word2Vec` module from `gensim`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "126c3ba3-c322-43f2-ae73-f0244a68b65f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07f07191-ce87-46a2-bd09-2729312f25a4",
   "metadata": {},
   "source": [
    "You can check out the documentation for this module [here](https://radimrehurek.com/gensim/models/word2vec.html#gensim.models.word2vec.Word2Vec). The main input to `Word2Vec` is a `sentences` argument, which consists of a list of lists: the outer list enumerates the documents, and the inner list enumerates the tokens within in each list. So, we need to run a word tokenizer on each of the tweets. Let's use `nltk`'s word tokenizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c5a6831-5b42-4ed3-99b8-3241b84a4785",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe2ca18-d18c-4248-924e-92fe5c9f97cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [word_tokenize(tweet) for tweet in tweets['text_processed']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a6e0abf-8a99-49f1-aaf5-ea21cb56cf50",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b273230a-5c31-4156-a183-53fdd7aecc78",
   "metadata": {},
   "source": [
    "Now, we train the model. We are going to use CBOW to train the model since it's better suited for smaller datasets. Take note of what other arguments we set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceaf88ad-677f-47de-bde5-cca609c7a243",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(\n",
    "    sentences=sentences,\n",
    "    vector_size=30,\n",
    "    window=5,\n",
    "    min_count=1,\n",
    "    sg=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7997232a-2337-4709-87c0-f8e40971d1a1",
   "metadata": {},
   "source": [
    "The model is now trained! Let's take a look at some word vectors. We can access them using the `wv` attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91bbdcaa-e04c-42fa-8b28-9b892f7abe89",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(model.wv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f1b2108-6022-4e78-ac4b-56bf80665d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv['worst']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc5b9a5f-3d8c-4e61-a24f-23ed9bda530d",
   "metadata": {},
   "source": [
    "Let's try running a `most_similar` query to see what we end up with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4578c24a-b913-4254-8a6c-ac4de77c878c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.most_similar('worst')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde0ac2f-bac5-40bf-8f3b-81031183c9da",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.most_similar('great')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a16554ab-06c8-4fc4-8d2f-1e134263bf8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.most_similar('united')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8e92543-087b-4ec6-b490-fb5f14dcec77",
   "metadata": {},
   "source": [
    "The `word2vec` model learned these relationships from the roughly 11,000 tweets in the corpus. These relationships look similar to some in the Google News word embeddings, have some differences that stem from the particular nature of the corpus, and the smaller number of documents."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de9b7584-91cc-43bf-8279-400c1bf3ccf1",
   "metadata": {},
   "source": [
    "## Challenge 3\n",
    "\n",
    "Try experimenting with different numbers of vector sizes, window sizes, and other parameters available in the `Word2Vec` module. Additionally, try training using skip-grams rather than CBOW."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d2bb2f9-1cd8-4e8c-b121-d087d7449bbe",
   "metadata": {},
   "source": [
    "# Classifying with Trained Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ce2d62-3d5b-4943-9266-e3f1241281cf",
   "metadata": {},
   "source": [
    "In the previous module, we used the airline tweets dataset to perform sentiment classification: we tried to classify the sentiment of a text given the bag-of-words representation. Can we do something similar with a word embedding representation?\n",
    "\n",
    "In the word embedding representation, we have an $N$-dimensional vector for each word in a tweet. How can we come up with a representation for the entire tweet?\n",
    "\n",
    "The simplest approach we could take is to simply average the vectors together to come up with a \"tweet representation\". Let's see how this works for predicting sentiment classification.\n",
    "\n",
    "First, we need to subset the dataset into the tweets which only have positive or negative sentiment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae7a379-1f02-417e-9dbd-76af895e50af",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_binary = tweets[tweets['airline_sentiment'] != 'neutral']\n",
    "y = tweets_binary['airline_sentiment']\n",
    "print(y.value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35def02f-4232-4643-b427-1fedbf472905",
   "metadata": {},
   "source": [
    "Now, we need to compute the feature matrix. We will query the word vector in each tweet, and come up with an average for the sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0588d51-6d9e-4566-919b-2d75fd4e9e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_size = 30\n",
    "X = np.zeros((len(y), vector_size))\n",
    "\n",
    "# Enumerate over tweets\n",
    "for idx, tweet in enumerate(tweets_binary['text_processed']):\n",
    "    # Tokenize the current tweet\n",
    "    tokens = word_tokenize(tweet)\n",
    "    n_tokens = len(tokens)\n",
    "    # Enumerate over tokens, obtaining word vectors\n",
    "    for token in tokens:\n",
    "        X[idx] += model.wv.get_vector(token)\n",
    "    # Take the average\n",
    "    X[idx] /= n_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78f75f96-ca46-421b-b5a6-7cd47af0c997",
   "metadata": {},
   "source": [
    "As before, we'll proceed with splitting the data into train/test examples. We'll bring back the logistic fitter function from before, with some small changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "654f81cb-a4ca-4c4d-a7b9-376b6b974988",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d0f37a2-0712-4d46-93ab-6bdacba845b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_logistic_regression(X, y):\n",
    "    \"\"\"Fits a logistic regression model to provided data.\"\"\"\n",
    "    model = LogisticRegressionCV(\n",
    "        Cs=10,\n",
    "        penalty='l2',\n",
    "        max_iter=1000,\n",
    "        cv=5,\n",
    "        refit=True).fit(X, y)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8126b42-f8ca-49af-8a8f-f243212478c9",
   "metadata": {},
   "source": [
    "We then run the fit, and evaluate it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f00d9f99-c44e-4b91-b905-32b70ce6674f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the logistic regression model\n",
    "fitter = fit_logistic_regression(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c59c916c-ba50-4336-b345-6e9ef29b27fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Training accuracy: {fitter.score(X_train, y_train)}\")\n",
    "print(f\"Test accuracy: {fitter.score(X_test, y_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40e7141f-2df5-4f1a-a356-aac82c44f982",
   "metadata": {},
   "source": [
    "While this performance is pretty good, it's definitely not as good as the bag-of-words representation we used in the previous module. There are few reasons this might be the case:\n",
    "\n",
    "1. We used a word embedding on a relatively small corpus. A word embedding obtained from a very large corpus would perform better. The tricky part in doing this is that our smaller corpus may have some niche tokens that are not in the larger model, so we'd have to work around that.\n",
    "2. We simply averaged word embeddings across tokens. When doing this, we lose meaning in the ordering of words. Other methods, such as `doc2vec`, have been proposed to address these concerns.\n",
    "3. Word embeddings might be an overly complicated approach for the task at hand. In a tweet aimed at an airline, a person needs to convey their sentiment in only 140 characters. So they are more likely to use relatively simple words that easily convey sentiment, making a bag-of-words a natural approach.\n",
    "\n",
    "It's important to note that we also lose out on the interpretability of the logistic regression model, because the actual dimensions of each word vector do not themselves have any meaning. \n",
    "\n",
    "Moral of the story: word embeddings are great, but always start with the simpler model! This is a good way to baseline other approaches, and it might actually work pretty well!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e443ff3f-bc6d-4190-b7aa-109d7909a242",
   "metadata": {},
   "source": [
    "## Challenge 4\n",
    "\n",
    "Write a function that performs the pipeline of building a `word2vec` model and constructing a design matrix. Use this function to try and see if you can change the performance of the model with other parameters (vector sizes, window sizes, etc.)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
