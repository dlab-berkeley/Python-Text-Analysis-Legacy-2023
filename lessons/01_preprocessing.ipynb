{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d3e7ea21-6437-48e8-a9e4-3bdc05f709c9",
   "metadata": {},
   "source": [
    "# Python Text Analysis: Fundamentals, Part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6493266-b5e3-4131-8837-9d2ab8c8dd02",
   "metadata": {},
   "source": [
    "In this workshop series, we'll establish building blocks for performing text analysis in Python. These techniques lie in the domain of *natural language processing*, where we apply computational techniques to language written by humans in order to explain some of the underlying structure.\n",
    "\n",
    "So, the million dollar question: How exactly do we go about performing computational methods on words?\n",
    "\n",
    "This is ultimately a question of *representations*. Text naturally is represented as words, which are understandable to humans because we have a grammatical and syntactical structure we use to extract meaning from those words. However, most machine learning and data science techniques utilize numerical methods to extract patterns from large datasets. So, we need to find a way to convert the language into a numerical representation. We'll start with this goal in mind, and demonstrate how involved this process can be.\n",
    "\n",
    "We'll start this process by first importing text into Python. Then, we'll cover a variety of preprocessing steps you might want to use before proceeding with computational methods. In the next sequence of this workshop, we'll work with the bag-of-words, or the first numerical representation of text that we'll encounter in this workshop series."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c8914f4-9783-4661-9cc9-32daca53e1fd",
   "metadata": {},
   "source": [
    "# Importing Text Files "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02b5a016-c4ac-433d-8a06-58ebbd0bd976",
   "metadata": {},
   "source": [
    "Text data we want to analyze will be stored in external files that need to be imported. These files will generally be text files (`.txt`) or comma separated value files (`.csv`).\n",
    "\n",
    "All the data used in this notebook are stored in a `data` folder that we need to access. We need to adjust our filepaths accordingly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "366544be-4b56-4ed2-ba8e-aa4e41d5f95b",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_path = '../data/sowing_and_reaping.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b462052f-766c-434c-a262-efa7326d61c0",
   "metadata": {},
   "source": [
    "## Text Files\n",
    "\n",
    "We'll first start by importing \"Sowing and Reaping\" by Frances Harper, which is stored in a text file. Python has built-in functionality for importing text files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3dc4536-5e6c-4052-a44d-c36760714925",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open and read the text\n",
    "with open(text_path, 'r') as file:\n",
    "    raw_text = file.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60b8e3b1-f92e-4525-a5ba-f396fb61271a",
   "metadata": {},
   "source": [
    "We've stored the text file in an object called `raw_text`. We'll remove the front and end matter for better preprocessing later:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff4469c-6a66-44bb-bd82-f20455497141",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the front and end matter\n",
    "sowing_and_reaping = raw_text[1114:684814]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "414ef3f4-1c0c-4c79-918f-72175a1292d3",
   "metadata": {},
   "source": [
    "## Challenge 1: Working with Strings\n",
    "\n",
    "* What type of object is `sowing_and_reaping`?\n",
    "* How many characters are in `sowing_and_reaping`?\n",
    "* How can we get the first 1000 characters of `sowing_and_reaping`?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d97e1de5-7cc6-4861-9ca9-0b58996d25f2",
   "metadata": {},
   "source": [
    "## Comma Separated Value (CSV) Files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b38f7db8-ecf6-4331-a26b-8a033f5731d0",
   "metadata": {},
   "source": [
    "Often, we may have data stored in \"dataframes\" or \"tables\", which consists of many samples (rows), each containing several features (columns). Among the features is likely a text column which contains the text of interest. These dataframes are often found as Comma Separated Value (CSV) files (and somewhat less frequently as tab separated value (TSV) files). In either case, there is some \"delimiter\" (i.e., a comma or tab) which helps separate entries from each other.\n",
    "\n",
    "The `pandas` package is the best package for dealing with dataframes in Python, and this package comes with its own function for reading CSV files. For example, let's read in a file containing many Tweets about airlines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c75449af-a3ad-48df-a64f-1325d2ef5c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import pandas\n",
    "import pandas as pd\n",
    "# Use pandas to import Tweets\n",
    "csv_path = '../data/airline_tweets.csv'\n",
    "tweets = pd.read_csv(csv_path, sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb6e3107-9f0a-414b-8267-1d5fbb09e246",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d201a3de-aaf6-4c21-97c8-5ac38517e0b5",
   "metadata": {},
   "source": [
    "Let's take a look at some of the Tweets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c32d507-ff14-4a2b-b7ae-613f3f76c69b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tweets['text'].iloc[0])\n",
    "print(tweets['text'].iloc[1])\n",
    "print(tweets['text'].iloc[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3972dfc7-ddb4-486e-ac48-d5a7593f7b91",
   "metadata": {},
   "source": [
    "## Challenge 2: Reading in Many Files\n",
    "\n",
    "The `data` folder contains another folder called `amazon`, which contains many `csv` files of Amazon reviews. Use a `for` loop to read in each dataframe. Do the following:\n",
    "\n",
    "* We've provided a path to the `amazon` folder, and a list of all the file names within the folder using the `os.listdir()` function.\n",
    "* Iterate over all these files, and import them using `pd.read_csv()`. You will need to use `os.path.join()` to create the correct path. Additionally, you need to provide `pandas` with the column names since they are not included in the reviews. We have create the `column_names` variable for you.\n",
    "* Extract the text column from each dataframe, and add then to the `reviews` list. \n",
    "* How many totals reviews do you obtain?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d9fa08c-7e28-4c3f-ab0c-1d1ab5ad4b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The os package has useful tools for file manipulation\n",
    "import os\n",
    "# Amazon review folder\n",
    "amazon_path = '../data/amazon'\n",
    "# List all the files in the amazon folder\n",
    "files = os.listdir(amazon_path)\n",
    "# Column names for each file\n",
    "column_names = ['id',\n",
    "                'product_id',\n",
    "                'user_id',\n",
    "                'profile_name',\n",
    "                'helpfulness_num',\n",
    "                'helpfulness_denom',\n",
    "                'score',\n",
    "                'time',\n",
    "                'summary',\n",
    "                'text']\n",
    "# Add each review text to this list\n",
    "reviews = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb680f3e-b82a-411c-aff0-79d2a6dfbc86",
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in files:\n",
    "    # Check that the file is actually a CSV file\n",
    "    if os.path.splitext(file)[1] == '.csv':\n",
    "        # YOUR CODE HERE\n",
    "        text = ''\n",
    "        reviews.extend(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cce98479-c234-41bc-a2da-c90ab822b421",
   "metadata": {},
   "source": [
    "There are other file types which you may come across: `json`, `xml`, `html`, etc. There are packages you can use to import each other these. The main challenge, in most cases, is dealing with multiple files, and extracting the actual text you want."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89e88f2b-bdaa-4c28-ad13-4e790ccb6827",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e967e45d-2cef-4c12-9584-95b8e196f02e",
   "metadata": {},
   "source": [
    "Our goal is to convert a text representation to a numerical representation. However, language can be messy. There's a variety of preprocessing steps that we'd like to do before we get to the numerical representation.\n",
    "\n",
    "We will largely be using a package called Natural Language Toolkit, or `nltk`, to perform these operations. In some cases, we'll use basic Python.\n",
    "\n",
    "There are a host of natural language processing packages one can use. For example, one newer package is `spaCy`, which is extremely powerful. Our goal here is to not make you an expert in a variety of NLP packages, but to expose you to principles that are shared by all of them. In this way, you'll be better prepared to open up any new NLP package you might have to use."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7fd3c0c-844b-4297-9edb-32bb441af9ec",
   "metadata": {},
   "source": [
    "## Installing `nltk`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e25bc2e-bd3d-488e-a9c4-7db18111ee5e",
   "metadata": {},
   "source": [
    "If this is your first time using `nltk`, we'll go through a couple steps to get set up. First, install `nltk` if you have not already:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23185480-b884-4c26-95f0-e9e647280b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run if you do not have nltk installed\n",
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3c7f258-c3b5-45a0-9de3-f0a9382bce8b",
   "metadata": {},
   "source": [
    "Next, we need to install a couple packages within `nltk`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff00fb9e-708b-43c8-9ec7-767228f34980",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3915028e-7ab1-4851-80f1-185da224cc52",
   "metadata": {},
   "source": [
    "## Text Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52910885-bcfa-4093-9c07-0634f6ea3200",
   "metadata": {},
   "source": [
    "\"Text cleaning\" is a catch-all term for the process of performing relatively simple tasks in order to normalize our code. Text cleaning can mean a variety of different things depending on your use case."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba11eee-c926-4591-bb32-d268646309ac",
   "metadata": {
    "tags": []
   },
   "source": [
    "### A Brief Introduction to Regular Expressions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b921d36-433d-452f-9d63-16c1050dfc02",
   "metadata": {},
   "source": [
    "Before we dive into the specific text cleaning processes, let's briefly introduce regular expressions. We do this here since many text cleaning steps may require regular expressions, and many NLP libraries heavily use them under the hood.\n",
    "\n",
    "Regular expressions (regexes) are a powerful way of searching for specific string patterns in large corpora. They have an infamously steep learning curve, but are very efficient when you get a handle on them.\n",
    "\n",
    "Our goal in this workshop is not to provide a deep (or even shallow) dive into regexes; instead, we want to expose you to them so that you're better prepared to do deep dives in the future.\n",
    "\n",
    "Regex testers are a useful tool in both understanding and creating regex expression. An example is this [website](https://regex101.com)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf750b5f-c08b-4f35-9eb8-b2182f892b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "pattern = 'test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd9690d3-c12f-4a2c-bcc7-cfa3f3523949",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_string = 'This is a test.'\n",
    "# Find tokens\n",
    "tokens = re.findall(pattern=pattern, string=test_string)\n",
    "print(tokens)\n",
    "# Replace tokens\n",
    "replaced = re.sub(pattern=pattern, repl='not a test', string=test_string) \n",
    "print(replaced)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d336ea3a-dd51-48a1-bf58-095298afdeb4",
   "metadata": {},
   "source": [
    "This is nice, but we could have done this somewhat easily with basic Python `string` functions. Let's try something more interesting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "244236dc-60b9-490c-98b7-33a6af5cb2b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word pattern matcher\n",
    "pattern = r'\\w+'\n",
    "re.findall(pattern, test_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e60923f-b981-4ad9-8777-0951105736d8",
   "metadata": {},
   "source": [
    "What did this do? Use the regex website to confirm your guess!\n",
    "\n",
    "For now, we won't go much further than this, but there are many resources online to continue learning about regexes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91ebae34-7b23-49a9-8aad-8f49cb55f320",
   "metadata": {},
   "source": [
    "### Lowercasing\n",
    "\n",
    "While there is often information in the \"casing\" of words (e.g., whether text is lowercase or uppercase), we often don't work in a regime where we're able to properly leverage this information. So, a common text cleaning step is to lowercase all text, in order to simplify our analysis.\n",
    "\n",
    "We can easily do this with the built-in string function `lower()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a07353a-0dfc-4438-ade1-d2fee7a3305f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sowing_and_reaping_lower = sowing_and_reaping.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e2cab72-6002-4861-af38-d0b67296eb62",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sowing_and_reaping[:200])\n",
    "print('------')\n",
    "print(sowing_and_reaping_lower[:200])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39654022-c93b-4bf5-9a63-efaf2b8125c3",
   "metadata": {},
   "source": [
    "### Removing Punctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7187439-ae54-4fa1-887d-f91921909cbe",
   "metadata": {},
   "source": [
    "Sometimes, you might want to keep only the alphanumeric characters (i.e. the letters and numbers) and ditch the punctuation. This becomes less common when we consider more advanced NLP algorithms. In many cases, you may do this step *after* tokenization, which we will discuss in the next section. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e11d9c-8440-479e-86a0-13c7e1e42d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "print(punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e1da6f-0c1b-4588-a310-028c18fcad9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuation_text = \"We've got quite a bit of punctuation here, don't we?!? #Python @D-Lab.\"\n",
    "no_punctuation = ''.join([char for char in punctuation_text if char not in punctuation])\n",
    "print(no_punctuation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a2f9bba-e2a5-4839-b017-f979a09752a1",
   "metadata": {},
   "source": [
    "### Stripping Blank Spaces"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99e7e296-d4ae-49f7-b899-2e078a2569f2",
   "metadata": {},
   "source": [
    "Removing blank space is a common step, as we might come across text with extraneous blank space. This is particularly common when text is imported from messy places, like webpages.\n",
    "\n",
    "Python has a built-in function to deal with blank space on the *ends* of strings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d42f85-71fd-4fb3-9c29-c24c41188fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "string = ' Hello! '\n",
    "string.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17f3d2ef-3111-43f4-b77d-08d9363a0756",
   "metadata": {},
   "source": [
    "What about within text? We will need to use a regular expression for this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca33c78-f8bf-438f-9737-f0a89c30c470",
   "metadata": {},
   "outputs": [],
   "source": [
    "example1_path = '../data/example1.txt'\n",
    "\n",
    "with open(example1_path, 'r') as file:\n",
    "    example1 = file.read()\n",
    "    \n",
    "print(example1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e94cb2-49c1-4be2-8496-6dd124d90188",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stripping only removes the ends\n",
    "print(example1.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "083b1553-dc87-4150-b857-76c2f0df8207",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A regular expression will handle blank spaces within the text\n",
    "blankspace_pattern = r'\\s+'\n",
    "blankspace_repl = ' '\n",
    "clean_text = re.sub(blankspace_pattern, blankspace_repl, example1)\n",
    "clean_text.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e2a168e-e2e0-46c0-b794-fa0ecae400dc",
   "metadata": {},
   "source": [
    "### Removing URLs, Hashtags, and Numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4320c777-984f-4c36-a479-229d060fdf22",
   "metadata": {},
   "source": [
    "Text containing non-alphabetic symbols may have additional meaning beyond simply using punctuation or numbers. For example, text may contain URLs, hashtags, or numbers. Each of these are informative in their own right.\n",
    "\n",
    "However, we rarely care about the exact URL used in a tweet. Similarly, we might not care about specific hashtags, or the precise number used. While, we could remove them completely, it's often informative to know that there *exists* a URL, hashtag, or number.\n",
    "\n",
    "So, we replace individual URLs, hashtags, and numbers with a \"symbol\" that preserves the fact these structures exist in the text. It's standard to just use the strings \"URL\", \"HASHTAG\", and \"DIGIT\".\n",
    "\n",
    "Since these types of text often contain precise structure, they're an apt case for using regular expressions. Let's apply these patterns to the Tweets above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8069337-4a39-4437-8710-f9b5c58335f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a Tweet with a URL in it\n",
    "url_tweet = tweets['text'].iloc[13]\n",
    "print(url_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc1fa10d-e546-4e61-89e5-adf53c7e6262",
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL \n",
    "url_pattern = r'https?:\\/\\/.*[\\r\\n]*'\n",
    "url_repl = ' URL '\n",
    "re.sub(url_pattern, url_repl, url_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40416f9e-4052-425f-9a00-94a3b0a94e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hashtag\n",
    "hashtag_pattern = r'(?:^|\\s)[ï¼ƒ#]{1}(\\w+)'\n",
    "hashtag_repl = ' HASHTAG '\n",
    "re.sub(hashtag_pattern, hashtag_repl, url_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba96052-a24e-46ca-8d99-60e47b485c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Digits\n",
    "digit_tweet = tweets['text'].iloc[32]\n",
    "print(digit_tweet)\n",
    "digit_pattern = '\\d+'\n",
    "digit_repl = ' DIGIT '\n",
    "re.sub(digit_pattern, digit_repl, digit_tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a288b9a-a33a-48be-bac4-93ffd39318ec",
   "metadata": {},
   "source": [
    "What other kinds of text strings can you think of that we might want to replace?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f5e64f-c6a0-447b-973e-a48e9eb80ade",
   "metadata": {},
   "source": [
    "## Challenge 3: Text Cleaning with Multiple Steps\n",
    "\n",
    "In Challenge 1, we imported many Amazon reviews, and stored them in a variable called `reviews`. Each element of the list is a string, representing the text of a single review. For each review:\n",
    "\n",
    "* Replace any URLs and digits.\n",
    "* Make all characters lower case.\n",
    "* Strip all blankspace.\n",
    "\n",
    "Keep in mind: the order in which you do these steps matters!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d599284-2392-488c-a4a8-dad5eeb50f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    # YOUR CODE HERE\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bacc7c15-1bc7-47cd-b668-ab57634c8b04",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c2b5c07-0232-47c5-b792-f074f815533f",
   "metadata": {},
   "source": [
    "One of the most important steps in text analysis is tokenization. This is the process of breaking down the text into \"tokens\", which are distinct chunks that we recognize as unique in whatever corpus we're working in.\n",
    "\n",
    "Let's start by importing an example file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd9d972-ae66-4415-8ce6-359cb9db0329",
   "metadata": {},
   "outputs": [],
   "source": [
    "example2_path = '../data/example2.txt'\n",
    "\n",
    "with open(example2_path) as file:\n",
    "    example2 = file.read()\n",
    "    \n",
    "print(example2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b402ed3-de7f-4e16-ade6-444c9e74f8eb",
   "metadata": {},
   "source": [
    "Let's try naively tokenizing by splitting up the text according to blankspace, using a basic Python string method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99fabb62-ff4e-48f7-b057-f72983dcf87e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = example2.split()\n",
    "# Print first ten tokens\n",
    "tokens[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60aa6b33-e927-4adc-8d6b-9a5a1f310e62",
   "metadata": {},
   "source": [
    "We can roughly think of this as \"word tokenization\". However, it's not always clear that simply splitting up by spaces will get what we want. Consider contractions, for example, which really consist of two words connected together. More advanced tokenizations will actually treat these words differently."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6263bf1f-b033-49c6-81b7-794d0e5bf1cb",
   "metadata": {},
   "source": [
    "`nltk` has a function called `word_tokenize` which can tokenize a string for us in an intelligent fashion. Ultimately, `nltk` basically is a bunch of regular expressions under the hood:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c63c1d-e1b3-4f21-aa57-460771e60801",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "nltk_tokens = word_tokenize(example2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f8769c1-6446-465b-a9b3-06e26086a330",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nltk_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9022de3b-ab72-4616-bba2-831f58491757",
   "metadata": {},
   "source": [
    "Looking at this example, you can see how `nltk` has made certain decisions about where and when to tokenize. Tokenization is critical for downstream processing, and there's a variety of methods for performing the tokenizing. Let's take a look at `spaCy`'s tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12cdd905-96ef-42f4-852f-a39ceede2a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install spaCy if necessary\n",
    "!pip install spacy\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f98810-7494-48bf-9d01-0c9df9bfc562",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import spaCy and load the dictionary\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "# Pass the example into the English pipeline\n",
    "doc = nlp(example2)\n",
    "spacy_tokens = [token.text for token in doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8acb52a-e49b-4fdd-b09c-2e6c4fd5d7d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare NLTK to spaCy\n",
    "print(nltk_tokens)\n",
    "print(spacy_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8e2147b-841e-4a1f-97f1-fd64f23eb118",
   "metadata": {},
   "source": [
    "## Challenge 4: Tokenizing a Large Text\n",
    "\n",
    "Tokenize \"Sowing and Reaping\", which we imported at the beginning of this workshop. Use a method of your choice.\n",
    "\n",
    "Once you've tokenized, find all the unique words types (you might want the `set` function). Then, sort the resulting `set` object to create a vocabulary (you might want to use the `sorted` function)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61259cad-3b35-4ce9-89b3-77ba8283bc18",
   "metadata": {},
   "source": [
    "## Removing Stop Words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee13e689-86cf-4561-91d2-9af97d86559a",
   "metadata": {},
   "source": [
    "Text often has words that are very common and usually not informative. These words tend to be pronouns or articles, such as \"the\", \"a\", \"it\", \"them\", etc. In many cases, these \"stop words\" are those that we may wish to remove before performing computation since they usually are not very informative. \n",
    "\n",
    "In practice, this is simple to do - we just filter out tokens by words. However, we may want to use different \"stop word lists\", depending on our use case. For example, `nltk` has a stop word list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7989a296-4f9c-44eb-b0f1-c554194fc7b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf9ff5f-73c8-446c-be8e-dabe1bd6d8a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What kinds of words are in the list?\n",
    "print(stop[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c49a372c-ecef-412f-bdf3-34f76f8b3f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove tokens that are stop words\n",
    "tokens = [token for token in tokens if token not in stop]\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d66cdf-89a4-49b6-812d-7215f97272d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare to the original text\n",
    "print(example2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd7692ad-8545-435f-b0cd-80f13ffff209",
   "metadata": {},
   "source": [
    "## Stemming and Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4ce5708-cd9d-46ff-a149-25799bb9f0f1",
   "metadata": {},
   "source": [
    "Stemming and lemmatization both refer to removing morphological affixes on words. Many words consist of a \"core\" word with a modified ending that adjusts the word's meaning in a given context. For example, the word \"grows\" is simply \"grow\" with an \"s\" added to denote a change in verb tense. In many cases, we're interested in the core content of the word. Stemming and lemmatization are the process of getting at the \"core\" of a word. This \"core\" component is often referred to as the *lemma*.\n",
    "\n",
    "Stemming is a rudimentary approach to obtaining the lemma: it simply removes an ending of a word. So, \"grows\" would be stemmed to \"grow\". The word \"running\" would be stemmed to \"run\".\n",
    "\n",
    "Lemmatization is more general: it aims to find the lemma of a word, but can handle cases where stemming may not work. For example, the word \"fairies\" cannot be stemmed to the lemma, \"fairy\". So, we need additional rules - provided by lemmatization - that can appropriately turn \"fairies\" into \"fairy\".\n",
    "\n",
    "`nltk` provides many algorithms for stemming. We'll use the Snowball Stemmer, which we'll import from `nltk`. We'll also look at the Word Net Lemmatizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "174b304d-5d5b-4303-9262-c43f6c6948e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import SnowballStemmer, WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e9e5f2e-2384-4631-b260-8fbe1e6970cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the stemmer and lemmatizer\n",
    "stemmer = SnowballStemmer('english')\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac5719f-83d2-49ef-b1a6-3c9cfe0a7ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stemming examples\n",
    "print(stemmer.stem('grows'))\n",
    "print(stemmer.stem('running'))\n",
    "print(stemmer.stem('coded'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35604e03-b724-4896-85be-7a664d18c27a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# When does stemming not quite work?\n",
    "print(stemmer.stem('fairies'))\n",
    "print(stemmer.stem('wolves'))\n",
    "print(stemmer.stem('abaci'))\n",
    "print(stemmer.stem('leaves'))\n",
    "print(stemmer.stem('carried'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d46f1f8-e6aa-48db-a954-508fdf4f542a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's try lemmatizing these, instead:\n",
    "print(lemmatizer.lemmatize('fairies'))\n",
    "print(lemmatizer.lemmatize('wolves'))\n",
    "print(lemmatizer.lemmatize('abaci'))\n",
    "print(lemmatizer.lemmatize('leaves'))\n",
    "print(lemmatizer.lemmatize('carried'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "044da5ed-cf6a-4114-b4b7-102e8504ce42",
   "metadata": {},
   "source": [
    "What happened with that last one? Sometimes we need to provide the lemmatizer a 'part-of-speech' tag to help resolve ambiguous cases. This is another argument in the lemmatizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50976d9b-5151-4dc1-862d-6cb2576c8dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lemmatizer.lemmatize('carried', pos='v'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d6c43f-33e9-4f48-8812-f77e31a85111",
   "metadata": {},
   "source": [
    "Try it with \"leaves\", which has more than one way to lemmatize!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "961a0ed8-d653-41c6-932d-90dba75329d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lemmatizer.lemmatize('leaves', pos='n'))\n",
    "print(lemmatizer.lemmatize('leaves', pos='v'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e2a4cf0-cfd9-4116-94ef-58ef4426b482",
   "metadata": {},
   "source": [
    "## Challenge 5: Apply a Lemmatizer to Text\n",
    "\n",
    "Lemmatize the tokenized `example2` text using the `nltk`'s `WordNetLemmatizer`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a429b117-1983-45bf-bdff-d6f0f83a8c22",
   "metadata": {},
   "source": [
    "## Challenge 6: Putting it All Together\n",
    "\n",
    "Write a function called `preprocess()` that accepts a string and performs the following preprocessing steps:\n",
    "\n",
    "* Lowercase text.\n",
    "* Replace all URLs and numbers with their respective tokens.\n",
    "* Strip blankspace.\n",
    "* Tokenize.\n",
    "* Remove punctuation.\n",
    "* Remove stop words.\n",
    "* Lemmatize the tokens.\n",
    "\n",
    "Apply this function to `sowing_and_reaping`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f5dbfb0-97b1-4abf-845c-17a0d4942831",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    # YOUR CODE HERE\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d259147-ad68-417c-99a6-1fecfb9b9a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess(sowing_and_reaping)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca49653f-8700-4681-a50a-43bbb14c124d",
   "metadata": {},
   "source": [
    "## Powerful Features of `spaCy`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "805c45a8-01e5-4fdb-86e6-2fab0e08543c",
   "metadata": {},
   "source": [
    "We will end this portion of the workshop by examining some of the more powerful features offered by the newer NLP library, `spaCy`. Beside being quite fast, `spaCy` provides very powerful built-in tools in its tokenizer. For example, we automatically get many of the above operations in one fell swoop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f741be70-2e03-4b95-b1a0-d5a525da9ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "short_example = \"We're learning about natural language processing at Berkeley.\"\n",
    "doc = nlp(short_example)\n",
    "\n",
    "for token in doc:\n",
    "    print(\n",
    "        f\"Token: {token.text}; Lemma: {token.lemma_}; Part-of-speech: {token.pos_}; \"\n",
    "        f\"Token shape: {token.shape_}; Alphabetical? {token.is_alpha}; Stop Word? {token.is_stop}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcec4cf7-eb3c-4b4a-9008-bb2c113634c8",
   "metadata": {},
   "source": [
    "Tokenizing, lemmatization, part of speech tagging, stop word detection, and a couple other things are provided to us up front when we pass a text into the `nlp` module."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c70884-da79-45e8-a3b1-905f57241817",
   "metadata": {},
   "source": [
    "`spaCy` also comes with some pretty shiny visualization tools:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab7351a4-b05f-4938-894f-9a21309b3473",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy import displacy\n",
    "displacy.render(doc, style=\"dep\", options={'compact': True})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f9e25d-7f90-4656-804e-351af0cf0356",
   "metadata": {},
   "source": [
    "For longer texts, we also get the ability to perform a variety of other operations very easily. Here are some cases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65eee367-1ade-4b95-b5bb-21968a9050b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "example3_path = '../data/example3.txt'\n",
    "\n",
    "with open(example3_path, 'r') as file:\n",
    "    example3 = file.read()\n",
    "    \n",
    "doc = nlp(example3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d59d76-d30c-44fb-b862-c208c3212e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(example3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ce243b-60b8-4e28-a69b-858e9df3bd55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentence segmentation\n",
    "print('Sentence Segmentation')\n",
    "for sentence in doc.sents:\n",
    "    print(sentence)\n",
    "\n",
    "# Entity detection\n",
    "print('\\nEntity Detection:')\n",
    "for entity in doc.ents:\n",
    "    print(entity.text, entity.label_)\n",
    "\n",
    "# Noun chunks\n",
    "print('\\nNoun Chunks:')\n",
    "for chunk in doc.noun_chunks:\n",
    "    print(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2f379d0-edf6-4282-9075-cc168b7444a3",
   "metadata": {},
   "source": [
    "There's a whole lot else we can do with it! Check out `spaCy`'s documentation to see more."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
