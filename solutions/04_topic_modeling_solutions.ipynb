{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd103089-a544-4396-9978-e879fc022d8d",
   "metadata": {},
   "source": [
    "# Python Text Analysis: Topic Modeling Solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98db9841-6916-48d0-8d43-487405b332a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.decomposition import LatentDirichletAllocation, NMF\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d87880-ec31-47d3-8bb4-cf2af3912f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "863f4a96-edc4-41bb-b9ce-44a847c709a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import fetcher function\n",
    "from sklearn.datasets import fetch_20newsgroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "952e2bc0-a009-476e-9658-07a83508d96e",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data, labels = fetch_20newsgroups(\n",
    "    subset='train',\n",
    "    shuffle=True,\n",
    "    random_state=1,\n",
    "    remove=(\"headers\", \"footers\", \"quotes\"),\n",
    "    return_X_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1723d1f5-1c98-41c3-a28c-c571b7d8ccb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_subsamples = 2000\n",
    "data = full_data[:n_subsamples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3206bfd7-ca4a-4a9a-a552-96299eab2664",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_top_words(model, feature_names, n_top_words=10, n_row=2, n_col=5, normalize=False):\n",
    "    \"\"\"Plots the top words from a topic model.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    model : topic model object (e.g., LatentDirichletAllocation, NMF)\n",
    "        The trained topic model. It should have a components_ attribute.\n",
    "    feature_names : array-like of strings\n",
    "        The names of each token, as a list or array.\n",
    "    n_top_words : int\n",
    "        The number of words to plot for each topic.\n",
    "    n_row : int\n",
    "        The number of rows in the plot.\n",
    "    n_col : int\n",
    "        The number of columns in the plot.\n",
    "    normalize : boolean\n",
    "        If True, normalizes the components so that they sum to 1 along samples.\n",
    "    \"\"\"\n",
    "    # Create figure\n",
    "    fig, axes = plt.subplots(n_row, n_col, figsize=(3 * n_col, 5 * n_row), sharex=True)\n",
    "    axes = axes.flatten()\n",
    "    components = model.components_\n",
    "    # Normalize components, if necessary\n",
    "    if normalize:\n",
    "        components = components / components.sum(axis=1)[:, np.newaxis]\n",
    "    # Iterate over each topic\n",
    "    for topic_idx, topic in enumerate(components):\n",
    "        # Obtain the top words for each topic\n",
    "        top_features_ind = topic.argsort()[: -n_top_words - 1 : -1]\n",
    "        # Get the token names\n",
    "        top_features = [feature_names[i] for i in top_features_ind]\n",
    "        # Get their values\n",
    "        weights = topic[top_features_ind]\n",
    "\n",
    "        # Plot the token weights as a bar plot\n",
    "        ax = axes[topic_idx]\n",
    "        ax.barh(top_features, weights, height=0.7)\n",
    "        ax.set_title(f\"Topic {topic_idx +1}\", fontdict={\"fontsize\": 20})\n",
    "        ax.invert_yaxis()\n",
    "        ax.tick_params(axis=\"both\", which=\"major\", labelsize=20)\n",
    "        \n",
    "        # Customize plot\n",
    "        for i in \"top right left\".split():\n",
    "            ax.spines[i].set_visible(False)\n",
    "\n",
    "    plt.subplots_adjust(top=0.90, bottom=0.05, wspace=0.90, hspace=0.3)\n",
    "\n",
    "    return fig, axes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a0c5325-4de3-4a67-890c-aecee563fbd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_tokens = 1000\n",
    "\n",
    "vectorizer = TfidfVectorizer(\n",
    "    max_df=0.95,\n",
    "    min_df=2,\n",
    "    max_features=n_tokens,\n",
    "    stop_words=\"english\")\n",
    "\n",
    "# Perform vectorizing\n",
    "tfidf = vectorizer.fit_transform(data)\n",
    "tokens = vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05a34af7-b163-4a79-bbc6-bda47f2e9962",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Challenge 1: Exploring Hyperparameters in NMF\n",
    "\n",
    "The choice of 10 components was somewhat arbitrary. It was something we had to do before we could proceed with fitting the model to the data. This is what's known as a *hyperparameter*. There are other hyperparameters in the `NMF`. For example, the `alpha` values specifies to what degree we should force values to be set equal to zero.\n",
    "\n",
    "Try fitting the NMF with other variations of hyperparameters, and plot the resulting topics using the `plot_top_words` function. What do you notice?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e87829cf-bd9d-457f-b276-e6d2fae34633",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make alpha very large\n",
    "n_components = 10\n",
    "random_state = 1\n",
    "\n",
    "nmf = NMF(\n",
    "    n_components=n_components,\n",
    "    random_state=random_state,\n",
    "    alpha=100,\n",
    "    l1_ratio=0.5,\n",
    "    init='nndsvda',\n",
    "    max_iter=500).fit(tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87955998-99b3-412b-acd5-baba58e1c8c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notice how everything is now zero\n",
    "fig, axes = plot_top_words(nmf, tokens)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "950b585d-0f27-401d-9e41-c3a0e085bb6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Increase number of topics\n",
    "n_components = 20\n",
    "random_state = 1\n",
    "\n",
    "nmf = NMF(\n",
    "    n_components=n_components,\n",
    "    random_state=random_state,\n",
    "    alpha=0.1,\n",
    "    l1_ratio=0.5,\n",
    "    init='nndsvda',\n",
    "    max_iter=500).fit(tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae70ebac-fefd-4303-b8e2-c90d07d936a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to change number of rows\n",
    "fig, axes = plot_top_words(nmf, tokens, n_row=4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8afd7135-243b-497f-a662-da08a43b6be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decrease number of topics\n",
    "n_components = 5\n",
    "random_state = 1\n",
    "\n",
    "nmf = NMF(\n",
    "    n_components=n_components,\n",
    "    random_state=random_state,\n",
    "    alpha=0.1,\n",
    "    l1_ratio=0.5,\n",
    "    init='nndsvda',\n",
    "    max_iter=500).fit(tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ba121b-d124-4fd9-a863-a32649f3b970",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to change number of rows\n",
    "fig, axes = plot_top_words(nmf, tokens, n_row=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "369f49ed-d1a2-4f84-9dc6-171481d1faf7",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Challenge 2: Exploring Hyperparameters in LDA\n",
    "\n",
    "As in the case of NMF, try performing LDA with other variations of hyperparameters, and plot the resulting topics using the `plot_top_words` function. Use the [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.LatentDirichletAllocation.html) as a guide to choose different hyperparameters.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27bc21ac-f5f7-4a90-8a33-b387a6703d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a CountVectorizer for LDA\n",
    "n_tokens = 1000\n",
    "count_vectorizer = CountVectorizer(\n",
    "    max_df=0.95,\n",
    "    min_df=2,\n",
    "    max_features=n_tokens,\n",
    "    stop_words=\"english\")\n",
    "counts = count_vectorizer.fit_transform(data)\n",
    "tokens = count_vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc06683e-0cc6-4f9e-87be-c12d1f10fb24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Increase number of components\n",
    "n_components = 20\n",
    "random_state = 0\n",
    "\n",
    "lda = LatentDirichletAllocation(\n",
    "    n_components=n_components,\n",
    "    max_iter=5,\n",
    "    learning_method=\"online\",\n",
    "    learning_offset=50.0, \n",
    "    random_state=random_state).fit(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab59bec-7f6d-4f8a-a28e-abe753a3e46b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change number of rows\n",
    "fig, axes = plot_top_words(lda, tokens, normalize=True, n_row=4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ea3f37-0fb5-4d79-9991-d4c4d51977d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decrease number of components\n",
    "n_components = 5\n",
    "random_state = 0\n",
    "\n",
    "lda = LatentDirichletAllocation(\n",
    "    n_components=n_components,\n",
    "    max_iter=5,\n",
    "    learning_method=\"online\",\n",
    "    learning_offset=50.0, \n",
    "    random_state=random_state).fit(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ea7441-f725-4e34-8c0f-8aefce3cc3bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change number of rows\n",
    "fig, axes = plot_top_words(lda, tokens, normalize=True, n_row=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f15cf1d-e256-4633-b93a-62f99ed94cb3",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Challenge 3: Finding Similar Documents\n",
    "\n",
    "Calculate the cosine similarity between all pairs of documents, and find the two documents whose cosine similarity is the highest. What are these documents? Do they seem similar?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2438ec8-cf86-4436-9d15-859bf0953cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_components = 10\n",
    "random_state = 0\n",
    "\n",
    "lda = LatentDirichletAllocation(\n",
    "    n_components=n_components,\n",
    "    max_iter=5,\n",
    "    learning_method=\"online\", # Use when dataset is large\n",
    "    learning_offset=50.0, \n",
    "    random_state=random_state).fit(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf0bf8b-7c9b-4761-9156-91da3a4d3319",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_representation = lda.transform(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e15a71-1225-448e-93af-ed60db6392cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate similarities\n",
    "similarities = cosine_similarity(topic_representation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61161d45-3b12-4a47-b59b-d9bc43323a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Double check the shape\n",
    "similarities.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4390c54-5105-4984-8b34-cd463cadac5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The diagonal of this matrix is all ones.\n",
    "# We want to zero this out in order to find the highest similarities.\n",
    "np.fill_diagonal(similarities, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2cdcdbc-8813-43bd-93f4-4ee0969adedd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, find the highest value\n",
    "# We need two functions: np.argmax, and np.unravel_index\n",
    "idx1, idx2 = np.unravel_index(np.argmax(similarities), similarities.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "202ce399-0a54-49fd-b597-779eb2af236d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What is the similarity?\n",
    "similarities[idx1, idx2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e31c1e48-8d3a-4931-b0c2-496c71f00036",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What are the documents?\n",
    "print(data[idx1])\n",
    "print(data[idx2])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
