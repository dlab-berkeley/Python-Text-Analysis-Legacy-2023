{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f340930-458d-4c63-939f-91ef765b8ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import seaborn as sns\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from string import punctuation\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b47991-a10b-4f6d-b4e9-188a4644d719",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import pandas\n",
    "# Use pandas to import tweets\n",
    "tweets_path = '../data/airline_tweets.csv')\n",
    "tweets = pd.read_csv(tweets_path, sep=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "984af902-74f5-4244-a511-08d33bdf6776",
   "metadata": {},
   "source": [
    "## Challenge 1: Getting to Know the Data\n",
    "\n",
    "Use `pandas` to find out the following about the airline tweets:\n",
    "\n",
    "* How many tweets are in the dataset?\n",
    "* How many tweets are positive, neutral, and negative?\n",
    "* What *proportion* of tweets are positive, neutral, and negative?\n",
    "* Make a bar plot showing the proportion of tweet sentiments.\n",
    "\n",
    "If you have time, try the following:\n",
    "\n",
    "* How much time separates the earliest and latest tweets?\n",
    "* What gets more retweets: positive, negative, or neutral tweets?\n",
    "* Identify the airline whose tweets have the highest proportion of negative sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f0a69c0-d541-4776-a4b7-9b5250a11db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many tweets are in the dataset?\n",
    "tweets.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf00946-3058-490e-bf31-74616e459ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many tweets are positive, neutral, and negative?\n",
    "tweets['airline_sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "235dd7fc-8105-446e-87c4-f8a43425dfe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What *proportion* of tweets are positive, neutral, and negative?\n",
    "tweets['airline_sentiment'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a04bbe-2fd0-4b88-bd72-a804c35575d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a bar plot showing the proportion of tweet sentiments\n",
    "sns.countplot(x=tweets['airline_sentiment'], order=['positive', 'neutral', 'negative'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f6c6c0-e3be-4507-8e05-a04c71d8a886",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How much time separates the earliest and latest tweets?\n",
    "sorted_by_time = pd.to_datetime(tweets['tweet_created'].sort_values())\n",
    "sorted_by_time.iloc[-1] - sorted_by_time.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f04d01-544e-4ffd-a3ae-1de68df7cb44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What gets more retweets: positive, negative, or neutral tweets?\n",
    "tweets.groupby('airline_sentiment')['retweet_count'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb5c45a-8fad-4515-b43c-629b52ab212c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Which airline receives the highest proportion of negative tweets?\n",
    "proportions = tweets.groupby(['airline', 'airline_sentiment']).size() / tweets.groupby('airline').size()\n",
    "proportions.unstack().sort_values('negative')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df76ff67-d285-4148-a642-f8bd4ce0ca4f",
   "metadata": {},
   "source": [
    "## Challenge 2: Creating a Preprocessing Pipeline for Social Media Data\n",
    "\n",
    "Write a function called `preprocess()` that performs the following on a text input:\n",
    "\n",
    "* Lowercase text.\n",
    "* Replace all URLs with the token \"URL\".\n",
    "* Replace all numbers with the token \"DIGIT\".\n",
    "* Replace hashtags with the token \"HASHTAG\".\n",
    "* Replace all users with the token \"USER\".\n",
    "* Remove blankspaces.\n",
    "\n",
    "We have provided regex patterns for each of the replacement steps in the following cells.\n",
    "\n",
    "Run your `preprocess()` function on `example_tweet` (two cells below), and when you think you have it working, apply it to the entire `text` column in the tweets DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8052135f-bbbb-4c4f-b090-ab08ec3d0ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply your function to the following example\n",
    "example_tweet = \"lol @justinbeiber and @BillGates are like soo 2000 #yesterday #amiright saw it on https://twitter.com #yolo\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa8ee2ff-94de-4352-afbb-b4ddfe44d35c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    \"\"\"Preprocesses a string.\"\"\"\n",
    "    # Lowercase\n",
    "    text = text.lower()\n",
    "    # Replace URLs\n",
    "    url_pattern = r'https?:\\/\\/.*[\\r\\n]*'\n",
    "    url_repl = ' URL '\n",
    "    text = re.sub(url_pattern, url_repl, text)\n",
    "    # Replace digits\n",
    "    digit_pattern = '\\d+'\n",
    "    digit_repl = ' DIGIT '\n",
    "    text = re.sub(digit_pattern, digit_repl, text)\n",
    "    # Replace hashtags\n",
    "    hashtag_pattern = r'(?:^|\\s)[ï¼ƒ#]{1}(\\w+)'\n",
    "    hashtag_repl = ' HASHTAG '\n",
    "    text = re.sub(hashtag_pattern, hashtag_repl, text)\n",
    "    # Replace users\n",
    "    user_pattern = r'@(\\w+)'\n",
    "    user_repl = ' USER '\n",
    "    text = re.sub(user_pattern, user_repl, text)\n",
    "    # Remove blank spaces\n",
    "    blankspace_pattern = r'\\s+'\n",
    "    blankspace_repl = ' '\n",
    "    text = re.sub(blankspace_pattern, blankspace_repl, text).strip()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e301605-cdb6-4c0e-9dfd-6f736400be33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on example tweet\n",
    "preprocess(example_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee42e6b0-6a04-4b40-bf4e-4c2856eacf78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply to text column to create a new column\n",
    "tweets['text_processed'] = tweets['text'].apply(lambda x: preprocess(x))\n",
    "tweets['text_processed'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e25a7b6-b726-46e7-96c1-a022eca35876",
   "metadata": {},
   "source": [
    "## Challenge 3: DTM Data Analysis\n",
    "\n",
    "* Print out the most infrequent words rather than the most frequent words. If you're not sure how, check the [documentation](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.sort_values.html)!\n",
    "* Print the average number of times each word is used in a tweet.\n",
    "* Which non-hashtag, non-digit token appears the most in any given tweet? How many times does it appear? What is the original tweet?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d7df60-776a-4c32-a701-9bef6b352486",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "counts = vectorizer.fit_transform(tweets['text_processed'])\n",
    "# Extract tokens\n",
    "tokens = vectorizer.get_feature_names_out()\n",
    "# Create DTM\n",
    "dtm = pd.DataFrame(data=counts.todense(),\n",
    "                   index=tweets.index,\n",
    "                   columns=tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5356fc1-da93-477f-8982-47a2a82f3ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Most infrequent tokens\n",
    "dtm.sum().sort_values(ascending=True).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c89580-601a-49b5-8037-dd1723d1ddf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average number of times each word is used in a tweet\n",
    "dtm.mean().sort_values(ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f088720b-98d7-4d0e-8e62-5b91a098428f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Which token appears the most in any given tweet?\n",
    "counts = pd.DataFrame()\n",
    "counts['token'] = dtm.idxmax(axis=1)\n",
    "counts['number'] = dtm.max(axis=1)\n",
    "counts[(counts['token'] != 'digit')\n",
    "       & (counts['token'] != 'hashtag')\n",
    "       & (counts['token'] != 'user')].sort_values(\n",
    "    'number',\n",
    "    ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb4a1da-8e1a-49df-85bb-aed1a97106a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at index 1214: \"worst\"\n",
    "tweets.iloc[1214]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53135016-92a6-4b6a-836f-0241234c3427",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at index 3915: \"lt\"\n",
    "tweets.iloc[3915]['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3840105-d53b-40f4-9a7e-90134b6b86d4",
   "metadata": {},
   "source": [
    "## Challenge 4: Customizing the Vectorizer with `nltk` inputs\n",
    "\n",
    "If you look at the `CountVectorizer` documentation, you'll see that it can actually accept a custom `tokenizer` and `stop_words` list. \n",
    "\n",
    "Using what you learned in the previous workshop, create a `CountVectorizer` that utilizes the `nltk` word tokenizer and stop word list. How does the resulting DTM look different?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3123645-1582-437d-92b2-db3233442efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get stop words\n",
    "stop_words = stopwords.words('english')\n",
    "# Create the vectorizer\n",
    "vectorizer = CountVectorizer(\n",
    "    lowercase=True,\n",
    "    tokenizer=word_tokenize,\n",
    "    stop_words=stop_words,\n",
    "    min_df=2,\n",
    "    max_df=0.95)\n",
    "# Fit, transform, and get tokens\n",
    "counts = vectorizer.fit_transform(tweets['text_processed'])\n",
    "tokens = vectorizer.get_feature_names_out()\n",
    "# Create dataframe\n",
    "dtm = pd.DataFrame(data=counts.todense(),\n",
    "                   index=tweets.index,\n",
    "                   columns=tokens)\n",
    "print(dtm.shape)\n",
    "dtm.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53695e2f-c113-4fb0-9ef0-a99523e5c132",
   "metadata": {},
   "source": [
    "## Challenge 5\n",
    "\n",
    "Try developing a **multinomial logistic regression** model, to predict positive, negative, and neutral labels. We've provided you a fitter function below, but it's up to you to create new labels, train-test splits, and perform the fitting and evaluation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c26312-5e13-4156-8af3-56107da98d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_multinomial_logistic_regression(X, y):\n",
    "    \"\"\"Fits a logistic regression model to provided data.\"\"\"\n",
    "    model = LogisticRegressionCV(\n",
    "        multi_class='multinomial',\n",
    "        Cs=10,\n",
    "        penalty='l1',\n",
    "        solver='saga',\n",
    "        tol=1e-2,\n",
    "        max_iter=50,\n",
    "        cv=3,\n",
    "        refit=True).fit(X, y)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b00245f4-1831-4e0e-96d6-2abf08ef0d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(max_features=5000)\n",
    "dtm = vectorizer.fit_transform(tweets['text_processed'])\n",
    "X = np.asarray(dtm.todense())\n",
    "y = tweets['airline_sentiment']\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a045313-e68d-4be2-8360-3f9af28cccc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "843b8b4a-8b77-4ac6-843d-45caeecad2bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This may take a while to run!\n",
    "model = fit_multinomial_logistic_regression(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce8f92c-4f2a-4c0d-ba4f-8ab50f9de90e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Training accuracy: {model.score(X_train, y_train)}\")\n",
    "print(f\"Test accuracy: {model.score(X_test, y_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b19e4f-fba6-46e3-8280-461582ca8624",
   "metadata": {},
   "source": [
    "## Challenge 6\n",
    "\n",
    "Create a new fitter function that uses a `RandomForestClassifier`. How is the performance? Check the [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8303ec28-d6a2-4700-8ee8-30a9195485f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_random_forest(X, y):\n",
    "    \"\"\"Fits a random forest model to provided data.\"\"\"\n",
    "    model = RandomForestClassifier(n_estimators=50).fit(X, y)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e55f53b0-fb95-4505-8fac-a0fa38630382",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_binary = tweets[tweets['airline_sentiment'] != 'neutral']\n",
    "vectorizer = TfidfVectorizer(max_features=5000)\n",
    "dtm = vectorizer.fit_transform(tweets_binary['text_processed'])\n",
    "X = np.asarray(dtm.todense())\n",
    "y = tweets_binary['airline_sentiment']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f81bc2-944f-47fa-adef-9e3ef840670a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = fit_random_forest(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02295745-0611-40c4-863a-8ef0c9d58882",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overfitting a bit!\n",
    "print(f\"Training accuracy: {model.score(X_train, y_train)}\")\n",
    "print(f\"Test accuracy: {model.score(X_test, y_test)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
