{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "270c7730-8f60-4449-9920-6030b6dcd1b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import spacy\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from pyprojroot import here\n",
    "from string import punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d90a7e63-04b7-4edb-bf3b-27a381d3d4ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use here to create the filepath\n",
    "text_path = here('data/sowing_and_reaping.txt')\n",
    "\n",
    "# Open and read the text\n",
    "with open(text_path, 'r') as file:\n",
    "    raw_text = file.read()\n",
    "    \n",
    "# Remove the front and end matter\n",
    "sowing_and_reaping = raw_text[1114:684814]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f446a0-61d7-4055-9acb-34b010f8572e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import pandas\n",
    "import pandas as pd\n",
    "# Use pandas to import Tweets\n",
    "csv_path = here('data/airline_tweets.csv')\n",
    "tweets = pd.read_csv(csv_path, sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe1682c8-491d-411d-858b-03bb1221481b",
   "metadata": {},
   "outputs": [],
   "source": [
    "example2_path = here('data/example2.txt')\n",
    "\n",
    "with open(example2_path) as file:\n",
    "    example2 = file.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd8599eb-1e4d-43db-b62c-dce158a18ce0",
   "metadata": {},
   "source": [
    "## Challenge 1: Working with `string`s\n",
    "\n",
    "* What type of object is `sowing_and_reaping`?\n",
    "* How many characters are in `sowing_and_reaping`?\n",
    "* How can we get the first 1000 characters of `sowing_and_reaping`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f8773e7-208c-4272-97e2-061c97860438",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(sowing_and_reaping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d3572a-9cad-4bbc-bc2a-cec92dcd044a",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sowing_and_reaping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c63cdfdf-e665-4c37-a755-8e110dbbdad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sowing_and_reaping[:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f57a143-8876-4305-b3f1-c77001296919",
   "metadata": {},
   "source": [
    "## Challenge 2: Reading in Many Files\n",
    "\n",
    "The `data` folder contains another folder called `amazon`, which contains many `csv` files of Amazon reviews. Use a `for` loop to read in each dataframe. Do the following:\n",
    "\n",
    "* We've provided a path to the `amazon` folder, and a list of all the file names within the folder using the `os.listdir()` function.\n",
    "* Iterate over all these files, and import them using `pd.read_csv()`. You will need to use `os.path.join()` to create the correct path. Additionally, you need to provide `pandas` with the column names since they are not included in the reviews. We have create the `column_names` variable for you.\n",
    "* Extract the text column from each dataframe, and add then to the `reviews` list. \n",
    "* How many totals reviews do you obtain?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f743402-ace4-4b3b-8175-ba8b7ae87197",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The os package has useful tools for file manipulation\n",
    "import os\n",
    "# Amazon review folder\n",
    "amazon_path = here('data/amazon')\n",
    "# List all the files in the amazon folder\n",
    "files = os.listdir(amazon_path)\n",
    "# Column names for each file\n",
    "column_names = ['id',\n",
    "                'product_id',\n",
    "                'user_id',\n",
    "                'profile_name',\n",
    "                'helpfulness_num',\n",
    "                'helpfulness_denom',\n",
    "                'score',\n",
    "                'time',\n",
    "                'summary',\n",
    "                'text']\n",
    "# Add each review text to this list\n",
    "reviews = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c052ec5-2f03-4c45-9f3e-ac0bbf29da52",
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in files:\n",
    "    # YOUR CODE HERE\n",
    "    full_path = os.path.join(amazon_path, file)\n",
    "    reviews_df = pd.read_csv(full_path, sep=',', names=column_names) \n",
    "    text = list(reviews_df['text'])\n",
    "    reviews.extend(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6be9a6e3-4658-4556-9bc6-40e58b2045cd",
   "metadata": {},
   "source": [
    "## Challenge 3: Text Cleaning with Multiple Steps\n",
    "\n",
    "In Challenge 1, we imported many Amazon reviews, and stored them in a variable called `reviews`. Each element of the list is a string, representing the text of a single review. For each review:\n",
    "\n",
    "* Strip all blank space\n",
    "* Make all characters lower case\n",
    "* Replace any URLs and digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f90b0867-06bb-4641-95b7-0eb584192b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    \"\"\"Preprocesses a string.\"\"\"\n",
    "    # Lowercase\n",
    "    text = text.lower()\n",
    "    # Replace URLs\n",
    "    url_pattern = r'https?:\\/\\/.*[\\r\\n]*'\n",
    "    url_repl = ' URL '\n",
    "    text = re.sub(url_pattern, url_repl, text)\n",
    "    # Replace digits\n",
    "    digit_pattern = '\\d+'\n",
    "    digit_repl = ' DIGIT '\n",
    "    text = re.sub(digit_pattern, digit_repl, text)\n",
    "    # Remove blank spaces\n",
    "    blankspace_pattern = r'\\s+'\n",
    "    blankspace_repl = ' '\n",
    "    text = re.sub(blankspace_pattern, blankspace_repl, text)\n",
    "    # Last step: strip\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4231fc31-0683-41f3-859c-6b6a3618c03a",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_reviews = [preprocess(review) for review in reviews]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00853c91-5118-4f2f-a575-c368c3534167",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(processed_reviews[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fcfa89a-b347-4c5d-b72c-238b901730f0",
   "metadata": {},
   "source": [
    "## Challenge 4: Tokenizing a Large Text\n",
    "\n",
    "Tokenize \"Sowing and Reaping\", which we imported at the beginning of this workshop. Use a method of your choice.\n",
    "\n",
    "Once you've tokenized, find all the unique words types (you might want the `set` function). Then, sort the resulting `set` object to create a vocabulary (you might want to use the `sorted` function)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e86f01-23b4-4eac-bd03-845b62e655b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk\n",
    "tokens = word_tokenize(sowing_and_reaping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d0a737-053c-4817-89bc-c06775ef22b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(sowing_and_reaping)\n",
    "tokens = [token.text for token in doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5697385-17b8-4498-b849-0c9abbe7b5d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_tokens = set(tokens)\n",
    "sorted_tokens = sorted(unique_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb8372c1-909d-4076-8f2b-2dc37b66bc6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sorted_tokens[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d08eb47-9f5b-4615-92c1-578379ec9835",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sorted_tokens[-100:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a5ef86b-3326-4342-919d-f7e7251d12c1",
   "metadata": {},
   "source": [
    "## Challenge 5: Apply a Lemmatizer to Text\n",
    "\n",
    "Lemmatize the tokenized `example2` text using the `nltk`'s `WordNetLemmatizer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ee90b1-619f-435e-8548-5deff33bf882",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0df0d22-48f0-45a3-997f-90b047e9473b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = word_tokenize(example2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d37407ca-a875-4107-9db3-213d21cc35ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatized = [lemmatizer.lemmatize(token) for token in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97eac648-f06e-4d5e-8097-4278fc4c3552",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lemmatized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec9b420-9050-40e7-9cbb-0961d99e3fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(example2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "390b9de5-7f14-48fa-b818-4e8cfe502d77",
   "metadata": {},
   "source": [
    "## Challenge 6: Putting it All Together\n",
    "\n",
    "Write a function called `preprocess()` that accepts a string and performs the following preprocessing steps:\n",
    "\n",
    "* Strip whitespace.\n",
    "* Lowercase text.\n",
    "* Tokenize.\n",
    "* Replace all URLs and numbers with the token \"DIGIT\".\n",
    "* Remove punctuation.\n",
    "* Remove stop words.\n",
    "* Lemmatize the tokens.\n",
    "\n",
    "Apply this function to `sowing_and_reaping`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f3154b-54a3-4458-8779-2859c9d594f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    \"\"\"Preprocesses a string.\"\"\"\n",
    "    # Lowercase\n",
    "    text = text.lower()\n",
    "    # Replace URLs\n",
    "    url_pattern = r'https?:\\/\\/.*[\\r\\n]*'\n",
    "    url_repl = ' URL '\n",
    "    text = re.sub(url_pattern, url_repl, text)\n",
    "    # Replace digits\n",
    "    digit_pattern = '\\d+'\n",
    "    digit_repl = ' DIGIT '\n",
    "    text = re.sub(digit_pattern, digit_repl, text)\n",
    "    # Remove blank spaces\n",
    "    blankspace_pattern = r'\\s+'\n",
    "    blankspace_repl = ' '\n",
    "    text = re.sub(blankspace_pattern, blankspace_repl, text).strip()\n",
    "    # Tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "    # Remove punctuation\n",
    "    tokens = [token for token in tokens if token not in punctuation]\n",
    "    # Remove stop words\n",
    "    stop = stopwords.words('english')\n",
    "    tokens = [token for token in tokens if token not in stop]\n",
    "    # Lemmatize\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3379311e-768e-4477-9879-c8eb9ad594b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed = preprocess(sowing_and_reaping)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
