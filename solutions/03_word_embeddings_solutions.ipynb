{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "37f37149-c4d4-4bb2-be40-33caccccdc92",
   "metadata": {},
   "source": [
    "# Python Text Analysis: Word Embeddings Solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "428bef9b-2ce9-456c-a260-9b72d79d6e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gensim\n",
    "import gensim.downloader as api\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegressionCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c923bb-7286-4d55-8002-684343a5a9cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "wv = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cf4a748-d43e-4c76-8eaa-ad504e5c3a51",
   "metadata": {},
   "source": [
    "## Challenge 1\n",
    "\n",
    "Look up the `doesnt_match` function in `gensim`'s documentation. Use this function to identify which word doesn't match in the following group:\n",
    "\n",
    "banana, apple, strawberry, happy\n",
    "\n",
    "Then, try it on groups of words that you choose. Here are some suggestions:\n",
    "\n",
    "1. A group of fruits, and a vegetable. Can it identify that the vegetable doesn't match?\n",
    "2. A group of vehicles that travel by land, and a vehicle that travels by air (e.g., a plane or helicopter). Can it identify the vehicle that flies?\n",
    "3. A group of scientists (e.g., biologist, physicist, chemist, etc.) and a person who does not study an empirical science (e.g., an artist). Can it identify the occupation that is not science based?\n",
    "\n",
    "To be clear, `word2vec` does not learn the precise nature of the differences between these groups. However, the semantic differences correspond to similar words appearing near each other in large corpora."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63c4f076-005a-4176-93dd-60e681f6b805",
   "metadata": {},
   "outputs": [],
   "source": [
    "wv.doesnt_match(['banana', 'apple', 'strawberry', 'happy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa15ad5-b3fe-4871-8c07-bfa1bac58e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "wv.doesnt_match(['banana', 'apple', 'strawberry', 'carrot'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe8838e-c48f-49f0-9a0c-f135e1fb7d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "wv.doesnt_match(['car', 'bike', 'bus', 'plane'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc36a9d-08c2-47ca-8564-0c3d76fde4b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "wv.doesnt_match(['biologist', 'physicist', 'chemist', 'artist'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fcbee67-bfce-41bb-8640-e0bbdc742519",
   "metadata": {},
   "source": [
    "## Challenge 2\n",
    "\n",
    "Carry out the following word analogies:\n",
    "\n",
    "1. Mouse : Mice :: Goose : ?\n",
    "2. Kangaroo : Joey :: Cat : ?\n",
    "3. United States : Dollar :: Mexico : ?\n",
    "4. Happy : Sad :: Up : ?\n",
    "5. California : Sacramento :: Canada : ?\n",
    "6. California : Sacramento :: Washington : ?\n",
    "\n",
    "What about something more abstract, such as:\n",
    "\n",
    "7. United States : hamburger :: Canada : ?\n",
    "\n",
    "Some work well, and others don't work as well. Try to come up with your own analogies!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e9c2318-2f3a-4a4f-937c-3968803656a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "wv.most_similar(positive=['mice', 'goose'], negative=['mouse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e77d3aec-a703-46bc-98fd-4cccf3f3f454",
   "metadata": {},
   "outputs": [],
   "source": [
    "wv.most_similar(positive=['joey', 'cat'], negative=['kangaroo'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fcd1a2a-4034-430c-82fe-7068ee335590",
   "metadata": {},
   "outputs": [],
   "source": [
    "wv.most_similar(positive=['Dollar', 'Mexico'], negative=['United_States'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e601ca0-3ec0-4458-a1bf-12dc8c21a1fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "wv.most_similar(positive=['sad', 'up'], negative=['happy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d887cadb-d4bc-4fc1-8a4f-f7b2bce74429",
   "metadata": {},
   "outputs": [],
   "source": [
    "wv.most_similar(positive=['Sacramento', 'Canada'], negative=['California'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de077690-f6db-4590-9e44-afd789ec67ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "wv.most_similar(positive=['Sacramento', 'Washington'], negative=['California'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a45aded-bf29-4f1b-8265-30cadd113bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "wv.most_similar(positive=['hamburger', 'Canada'], negative=['United_States'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a2a737-4617-4288-b09f-85a5d1139697",
   "metadata": {},
   "source": [
    "## Challenge 3\n",
    "\n",
    "Try experimenting with different numbers of vector sizes, window sizes, and other parameters available in the `Word2Vec` module. Additionally, try training using skip-grams rather than CBOW."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70d6c037-133b-4979-9aaa-830e11e29b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_path = '../data/airline_tweets.csv')\n",
    "tweets = pd.read_csv(tweets_path, sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c63a20-da4f-4e3f-8ec2-2ee89a736d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    \"\"\"Preprocesses a string.\"\"\"\n",
    "    # Lowercase\n",
    "    text = text.lower()\n",
    "    # Replace URLs\n",
    "    url_pattern = r'https?:\\/\\/.*[\\r\\n]*'\n",
    "    url_repl = ' URL '\n",
    "    text = re.sub(url_pattern, url_repl, text)\n",
    "    # Replace digits\n",
    "    digit_pattern = '\\d+'\n",
    "    digit_repl = ' DIGIT '\n",
    "    text = re.sub(digit_pattern, digit_repl, text)\n",
    "    # Replace hashtags\n",
    "    hashtag_pattern = r'(?:^|\\s)[ï¼ƒ#]{1}(\\w+)'\n",
    "    hashtag_repl = ' HASHTAG '\n",
    "    text = re.sub(hashtag_pattern, hashtag_repl, text)\n",
    "    # Replace users\n",
    "    user_pattern = r'@(\\w+)'\n",
    "    user_repl = ' USER '\n",
    "    text = re.sub(user_pattern, user_repl, text)\n",
    "    # Remove blank spaces\n",
    "    blankspace_pattern = r'\\s+'\n",
    "    blankspace_repl = ' '\n",
    "    text = re.sub(blankspace_pattern, blankspace_repl, text).strip()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5150121-08fe-4e50-8fa4-19fcbe96639c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets['text_processed'] = tweets['text'].apply(lambda x: preprocess(x))\n",
    "tweets['text_processed'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e001db47-d08d-471d-a841-ffa849462a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [word_tokenize(tweet) for tweet in tweets['text_processed']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ae4512-cdd6-41e4-b798-ce538ded8010",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(\n",
    "    sentences=sentences,\n",
    "    vector_size=50,\n",
    "    window=5,\n",
    "    min_count=2,\n",
    "    sg=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4129c620-8120-4634-87dd-790820de6928",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.most_similar('worst')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a5d01f9-2076-4820-9cf0-a578925fd9a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.most_similar('great')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72fdf27f-0212-4d76-a015-fc9bd29c1192",
   "metadata": {},
   "source": [
    "## Challenge 4\n",
    "\n",
    "Write a function that performs the pipeline of building a `word2vec` model and constructing a design matrix. Use this function to try and see if you can change the performance of the model with other parameters (vector sizes, window sizes, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c2058d-acfd-475e-a6e0-6d1555e773e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_binary = tweets[tweets['airline_sentiment'] != 'neutral']\n",
    "y = tweets_binary['airline_sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44f6648b-4eaf-4a6f-a023-ff5b25bd880f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def featurizer(documents, to_train, vector_size=50, window=6, sg=0):\n",
    "    \"\"\"Computes a feature matrix from a document corpus.\"\"\"\n",
    "    sentences = [word_tokenize(doc) for doc in documents]\n",
    "    # Train word2vec\n",
    "    model = Word2Vec(\n",
    "        sentences=sentences,\n",
    "        vector_size=vector_size,\n",
    "        window=window,\n",
    "        min_count=1,\n",
    "        sg=sg)\n",
    "    \n",
    "    X = np.zeros((len(to_train), vector_size))\n",
    "    # Enumerate over tweets\n",
    "    for idx, doc in enumerate(to_train):\n",
    "        # Tokenize the current tweet\n",
    "        tokens = word_tokenize(doc)\n",
    "        n_tokens = len(tokens)\n",
    "        # Enumerate over tokens, obtaining word vectors\n",
    "        for token in tokens:\n",
    "            X[idx] += model.wv.get_vector(token)\n",
    "        # Take the average\n",
    "        X[idx] /= n_tokens\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27785ffa-0fce-4b8c-8e36-0fc1573810ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = featurizer(tweets['text_processed'], tweets_binary['text_processed'], vector_size=80, window=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71a3da82-91b0-45a9-a7fb-76832ed1315c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f52dc37-f051-45bc-9b09-bc61d46960da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_logistic_regression(X, y):\n",
    "    \"\"\"Fits a logistic regression model to provided data.\"\"\"\n",
    "    model = LogisticRegressionCV(\n",
    "        Cs=5,\n",
    "        penalty='l2',\n",
    "        max_iter=1000,\n",
    "        tol=1e-2,\n",
    "        cv=3,\n",
    "        refit=True).fit(X, y)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e148d1-952e-4ff4-84ac-5f020ecfe29b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the logistic regression model\n",
    "fitter = fit_logistic_regression(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a67ce823-5899-4336-958a-fa7cabcc40c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Training accuracy: {fitter.score(X_train, y_train)}\")\n",
    "print(f\"Test accuracy: {fitter.score(X_test, y_test)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
